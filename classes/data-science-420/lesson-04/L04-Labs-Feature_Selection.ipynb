{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5; padding: 15px 5px; \" >\n",
    "<p>Use this notebook to follow along with the lab tutorial.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">Lesson 4 Feature Engineering and Selection</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 Handling Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data from CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file = \"https://library.startlearninglabs.uw.edu/DATASCI420/Datasets/Tennis.csv\"\n",
    "data = pd.read_csv(file, header=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "X = data[[\"outlook\",\"temp\",\"humidity\",\"windy\"]]\n",
    "le =  ce.OneHotEncoder(return_df=False,impute_missing=False,handle_unknown=\"ignore\")\n",
    "X_encoded = le.fit_transform(X)\n",
    "X_encoded[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the category mapping\n",
    "le.category_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding by Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_encoded = pd.get_dummies(data, columns=[\"outlook\",\"temp\",\"humidity\",\"windy\"], \\\n",
    "                             prefix=[\"outlook\",\"temp\",\"humdity\",\"windy\"])\n",
    "data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References for More Complete List of One-hot Encoding Methodologies\n",
    "\n",
    "Moffitt, C. (2017) <a href=\"http://pbpython.com/categorical-encoding.html\">Guide to Encoding Categorical Values in Python</a>, Practical Business Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Values of Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "General_Prob = (data.iloc[:,4] == \"yes\").sum()/float(data.shape[0])\n",
    "variable_risks = {}\n",
    "for variable in ['outlook', 'temp', 'humidity', 'windy']:\n",
    "    tab = pd.crosstab(data[variable], data.play) #Create a contingency table\n",
    "    print(tab)\n",
    "    num_levels = tab.shape[0]\n",
    "    level_risk = {}\n",
    "    levels = list(tab.index)\n",
    "    level_index = 0\n",
    "    for lev in levels:\n",
    "        if lev == True:\n",
    "            lev = 'True'\n",
    "        elif lev == False:\n",
    "            lev = 'False'\n",
    "        # general_prob is used as asmooth parameters when calculating risk values\n",
    "        level_risk[lev] = np.log((tab.iloc[level_index, 1] + General_Prob) / (tab.iloc[level_index, 0] + 1 - General_Prob))\n",
    "        level_index += 1\n",
    "    variable_risks[variable] = level_risk\n",
    "print(variable_risks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace the Original Categorical Values with Risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_risks = data.copy()\n",
    "# data_risks.windy = data_risks.windy.astype(float)\n",
    "num_obs = data.shape[0]\n",
    "for variable in ['outlook', 'temp', 'humidity', 'windy']:\n",
    "    for i in range(num_obs):\n",
    "        if data[variable][i] == True:\n",
    "            lev = 'True'\n",
    "        elif data[variable][i] == False:\n",
    "            lev = 'False'\n",
    "        else:\n",
    "            lev = data[variable][i]\n",
    "        data_risks[variable][i] = variable_risks[variable][lev]\n",
    "data_risks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Recency, Frequency, and Monetary (RFM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file = \"https://library.startlearninglabs.uw.edu/DATASCI420/Datasets/Retail_Churn_Data.csv\"\n",
    "data = pd.read_csv(file, sep=\",\", header=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Timestamp to Datetime Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "data[\"Timestamp\"] = pd.to_datetime(data[\"Timestamp\"], format='%m/%d/%Y %H:%M')\n",
    "data.head()\n",
    "print(\"Minimal Date=%s, Maximal Date=%s\"%(min(data[\"Timestamp\"]).strftime(\"%Y-%m-%d %H:%M\"), \\\n",
    "                                          max(data[\"Timestamp\"]).strftime(\"%Y-%m-%d %H:%M\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate RFM Features for Users at Each Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Start_Date_Obj = dt.datetime.strptime(\"1/1/2001\", \"%m/%d/%Y\")\n",
    "End_Date_Obj = dt.datetime.strptime(\"1/10/2001\", \"%m/%d/%Y\")\n",
    "Time_Window = 60 #days. Only consider customers who have activities within the recent 60 days\n",
    "FM_Window = 7 #days for frequency and monetary\n",
    "\n",
    "check_point_date = Start_Date_Obj\n",
    "UserID = []\n",
    "Checkpoint = []\n",
    "Recency = []\n",
    "Frequency = []\n",
    "Monetary_Value = []\n",
    "Monetary_Quantity = []\n",
    "while check_point_date <= End_Date_Obj:\n",
    "    window_start = check_point_date - dt.timedelta(days = Time_Window)\n",
    "    mask = (data[\"Timestamp\"] >= window_start) & (data[\"Timestamp\"] < check_point_date)\n",
    "    # Get the data in [checkpoint-60days, checkpoint]\n",
    "    data_checkpoint = data.loc[mask]\n",
    "    # Get the ids of users who have activities in [checkpoint-60days, checkpoint]\n",
    "    unique_users = list(set(data_checkpoint[\"UserId\"]))\n",
    "    print(\"There are %d unique users.\"%(len(unique_users)))\n",
    "    FM_Window_Start = check_point_date - dt.timedelta(days = FM_Window)\n",
    "    for user in unique_users:\n",
    "        UserID.append(user)\n",
    "        Checkpoint.append(check_point_date)\n",
    "        mask = data_checkpoint[\"UserId\"] == user\n",
    "        data_checkpoint_user = data_checkpoint.loc[mask]\n",
    "        delta = check_point_date - max(data_checkpoint_user[\"Timestamp\"])\n",
    "        recency = delta.days #Recency, days between checkpoint and last transaction time\n",
    "        mask = data_checkpoint_user[\"Timestamp\"] >= FM_Window_Start\n",
    "        data_checkpoint_user_fm = data_checkpoint_user.loc[mask]\n",
    "        frequency = data_checkpoint_user_fm.shape[0]\n",
    "        value = np.sum(data_checkpoint_user_fm.iloc[:, 8]) #monetary values\n",
    "        quantity = np.sum(data_checkpoint_user_fm.iloc[:, 7])#monetary quantity\n",
    "        Recency.append(recency)\n",
    "        Frequency.append(frequency)\n",
    "        Monetary_Value.append(value)\n",
    "        Monetary_Quantity.append(quantity)\n",
    "    check_point_date = check_point_date + dt.timedelta(days = 1)\n",
    "# Consolidate all columns into a signle data frame\n",
    "RFM_Dict = OrderedDict([ ('UserID', UserID),\n",
    "          ('Checkpoint', Checkpoint),\n",
    "          ('Recency',  Recency),\n",
    "          ('Frequency', Frequency),\n",
    "          ('Value', Monetary_Value),\n",
    "          ('Quantity', Monetary_Quantity)] )\n",
    "RFM_Frame = pd.DataFrame.from_dict(RFM_Dict)\n",
    "RFM_Frame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. Filter-based Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.linspace(start = -1, stop = 3, num=401, endpoint=True)\n",
    "Y = X**2 - 2*X + 1\n",
    "print(\"Standard Deviation of Y=%.2f\"%np.std(Y))\n",
    "\n",
    "noise_var = 0.1\n",
    "noise = np.random.normal(0, noise_var, len(X))\n",
    "Y += noise\n",
    "#Y = noise\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X, Y)\n",
    "plt.show()\n",
    "\n",
    "# Calculation Correlation\n",
    "corr = np.corrcoef(X, Y)[0, 1]\n",
    "print(\"Correlation between X and Y is %.2f\"%corr)\n",
    "# Calculate Mutual Information\n",
    "\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def calc_MI(x, y, bins):\n",
    "    c_xy = np.histogram2d(x, y, bins)[0]\n",
    "    mi = mutual_info_score(None, None, contingency=c_xy)\n",
    "    return mi\n",
    "\n",
    "mi = calc_MI(X, Y, 20)\n",
    "print(\"Mutual information=%.2f\"%mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4. Stepwise and Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise Model Selection\n",
    "\n",
    "#### Backward model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFE #Recursive Feature Elimination\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    "print(X[0:10,:]) # print out the first 10 rows\n",
    "estimator = LinearRegression()\n",
    "selector = RFE(estimator, 5, step=1)#select 5 features. Step=1 means each step only remove 1 variable from the model\n",
    "selector = selector.fit(X, y)\n",
    "print(selector.support_) # The mask of selected features.\n",
    "print(selector.ranking_) # selected features are ranked 1. The 6th is the one that is removed first,\n",
    "                         # 2nd is the one that is removed last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Machine Learning Extensions\n",
    "See <a href=\"http://rasbt.github.io/mlxtend/\">mlxtend's documentation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to run\n",
    "# !pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Forward Stepwise Feature Selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "print(X[0:10, :])\n",
    "lr = LinearRegression()\n",
    "\n",
    "sfs = SFS(lr, \n",
    "          k_features=13, # k_features has to be smaller or equal to the number of features. If equal to, it starts from\n",
    "                         # intercept to the full model\n",
    "          forward=True,  # forward\n",
    "          floating=False, \n",
    "          scoring='neg_mean_squared_error',\n",
    "          cv=10)\n",
    "\n",
    "sfs = sfs.fit(X, y)\n",
    "fig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\n",
    "\n",
    "plt.title('Sequential Forward Selection (w. StdErr)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.get_metric_dict()[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO and Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO\n",
    "from sklearn import linear_model\n",
    "\n",
    "alpha = 0.5 # Increasing alpha can shrink more variable coefficients to 0\n",
    "clf = linear_model.Lasso(alpha=alpha)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.coef_)\n",
    "\n",
    "print(clf.intercept_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "from sklearn import linear_model\n",
    "alpha = 10 \n",
    "clf = linear_model.Ridge(alpha=alpha)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.coef_)\n",
    "\n",
    "print(clf.intercept_)\n",
    "\n",
    "import numpy as np\n",
    "# Increasing alpha can compress the L2 norm of the coefficients to 0 (but not selecting variables)\n",
    "print(\"Sum of square of coefficients = %.2f\"%np.sum(clf.coef_**2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5; padding: 15px 5px; \" >\n",
    "<p>For additional practice, please see the Workshop notebooks.</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
