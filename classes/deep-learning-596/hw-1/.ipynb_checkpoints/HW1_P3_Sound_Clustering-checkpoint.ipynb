{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW1_P3_Sound_Clustering.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"o44-fIiD7oO2"},"source":["#Problem 3: Sound Clustering\n","\n","by Haotian Zhang"]},{"cell_type":"markdown","metadata":{"id":"sWBVY24N75TL"},"source":["In sound processing, the [mel-frequency cepstrum](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum#:~:text=Mel%2Dfrequency%20cepstral%20coefficients%20(MFCCs,%2Da%2Dspectrum%22).) (MFC) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency."]},{"cell_type":"markdown","metadata":{"id":"o-PhDbvj8ItE"},"source":["**Mel-frequency cepstral coefficients** (MFCCs) are coefficients that collectively make up an MFC. MFCCs are commonly used as features in speech recognition systems, such as the systems which can automatically recognize numbers spoken into a telephone. MFCCs are commonly derived as follows:\n","1. Take the [Fourier transform](https://en.wikipedia.org/wiki/Fourier_transform) of (a windowed excerpt of) a signal.\n","2. Map the powers of the spectrum obtained above onto the mel scale, using [triangular overlapping windows](https://en.wikipedia.org/wiki/Window_function#Triangular_window).\n","3.\tTake the logs of the powers at each of the mel frequencies.\n","4.\tTake the [discrete cosine transform](https://en.wikipedia.org/wiki/Discrete_cosine_transform) of the list of mel log powers, as if it were a signal.\n","5.\tThe MFCCs are the amplitudes of the resulting spectrum.\n","\n","Sounds scary and tedious? No worries. we will help you go through a simple process using Python to do the `feature extraction` for sound (music, speech, etc.) and then `classify` the audio signal into different clusters. \n"]},{"cell_type":"markdown","metadata":{"id":"HucG1G8r8qy5"},"source":["### Feature Extraction "]},{"cell_type":"markdown","metadata":{"id":"zjNXk5wW84So"},"source":["**Extraction of features is a very important part in analyzing and finding relations between different things**. The data provided of audio cannot be understood by the models directly to convert them into an understandable format feature extraction is used. It is a process that explains most of the data but in an understandable way. Feature extraction is required for classification, prediction and recommendation algorithms.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UcZjRYxB9CAA"},"source":["In P1, we will first extract features of animal sound files that will help us to classify the sound into different clusters. Let’s get familiar with the audio signal first. The audio signal is a 3-dimensional signal in which the three axes represent the time, amplitude and frequency.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mIoh_XOA9H1i"},"source":["We will be using [librosa](https://librosa.github.io/librosa/) for analyzing and extracting features of an audio signal. For playing audio, we will use [pyAudio](https://people.csail.mit.edu/hubert/pyaudio/docs/) so that we can play music directly on Colab. Download three audio files (`Bluejay.mp3`, `Dove.mp3` and `Ducks.wav`) provided on Canvas webapge. Upload your files by clicking `Files -> Upload to your session storage`."]},{"cell_type":"code","metadata":{"id":"vyRmQAIV_qZ9"},"source":["# let's install librosa and pyAudio first!\n","!pip install librosa\n","!pip install numba==0.48\n","\n","!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n","!pip install pyaudio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JyuSWC6W7EBX"},"source":["# Loading an audio\n","# let's take bluejay.mp3 as an example\n","import librosa\n","\n","audio_path = \"Bluejay.mp3\"\n","x , sr = librosa.load(audio_path)\n","print(type(x), type(sr))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HzgiAcxxA3it"},"source":["`.load` loads an audio file and decodes it into a 1-dimensional array which is a time series `x` , and `sr` is a sampling rate of `x` . Default `sr` is 22kHz. We can override the `sr` by"]},{"cell_type":"code","metadata":{"id":"gMr5GTOBA-h8"},"source":["librosa.load(audio_path, sr=44100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6B9fVa03BCXz"},"source":["We can also disable sampling by:"]},{"cell_type":"code","metadata":{"id":"N_ZGoGPUBF7L"},"source":["librosa.load(audio_path, sr=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PUMXUX9PB4BA"},"source":["# Playing an audio\n","import IPython.display as ipd\n","ipd.Audio(audio_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rHrb60I-CH75"},"source":["`IPython.display` allow us to play audio on jupyter notebook directly. It has a very simple interface with some basic buttons."]},{"cell_type":"code","metadata":{"id":"8p21KM-ACRzT"},"source":["#display waveform\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import librosa.display\n","plt.figure(figsize=(14, 5))\n","librosa.display.waveplot(x, sr=sr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dqrQXD4nCX33"},"source":["`librosa.display` is used to display the audio files in different formats such as wave plot, spectrogram, or colormap. Waveplots let us know the loudness of the audio at a given time. Spectogram shows different frequencies playing at a particular time along with its amplitude. Amplitude and frequency are important parameters of the sound and are unique for each audio. `librosa.display.waveplot` is used to plot waveform of amplitude vs. time where the first axis is an amplitude and second axis is time."]},{"cell_type":"markdown","metadata":{"id":"pEZYUj7GFg6b"},"source":["**MFCC - Mel-Frequency Cepstral Coefficients**\n","\n","This feature is one of the most important method to extract a feature of an audio signal and is used majorly whenever working on audio signals. The MFCCs of a signal are a small set of features (usually about 10–20) which concisely describe the overall shape of a spectral envelope."]},{"cell_type":"code","metadata":{"id":"vkqGtTHlF5a8"},"source":["# MFCC — Mel-Frequency Cepstral Coefficients\n","mfccs = librosa.feature.mfcc(x, sr=sr)\n","print(mfccs.shape)\n","# Displaying the MFCCs:\n","librosa.display.specshow(mfccs, sr=sr, x_axis='time')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J9IQNpatGMmD"},"source":["`.mfcc` is used to calculate mfccs of a signal.\n","\n","By printing the shape of mfccs you get how many mfccs are calculated on how many frames. The first value represents the number of mfccs calculated and another value represents a number of frames available."]},{"cell_type":"markdown","metadata":{"id":"5exfGDq3HG6C"},"source":["#### Questions\n","*  **Q1**: Do the framing of Bird sounds using A=20ms windows, B=10ms, and A-B=10ms of overlapping. 4 seconds of Bird sounds will generate 399 frames.\n","*  **Q2**: Generate 13 MFCC coefficients for every frame. Every 4 sec of Bird sound will have 399x13 MFCC coefficients matrix as a result.\n","*  **Q3**: Plot the 399x13 MFCC coefficients for all three Bird sounds in Python.\n","\n","Here I provide some helpful functions to use."]},{"cell_type":"code","metadata":{"id":"d5VXkAPcHusb"},"source":["# TODO: Your code here. (You may use multiple code and text segments to display your solutions.)\n","# Q1\n","# ...\n","# Q2\n","# ...\n","# Q3\n","# ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RPBeyo8rKiQN"},"source":["# This is a fixed SampleRateSetting\n","_SAMPLING_FREQ = 12000\n","\n","def _get_audio(audio_path):\n","    # sr=None disables dynamic resampling\n","    x, sr = librosa.load(audio_path, sr=_SAMPLING_FREQ)\n","    print(f'Loaded {audio_path} (sampling rate {sr})')\n","    \n","    return x, sr\n","\n","def _display_audio(x, sr):\n","    # Show audio to play in Jupyter\n","    ipd.display(ipd.Audio(x, rate=sr))\n","\n","def _compute_mfcc(x, sr, N_frames=399, Tw=20, Ts=10, alpha=0.97, R=(300, 3700), M=20, C=13, L=22):\n","    \"\"\"\n","    Compute MFCCs\n","    \n","    This is a rough re-implementation of HTK MFCC MATLAB using librosa:\n","    https://www.mathworks.com/matlabcentral/fileexchange/32849-htk-mfcc-matlab?focused=5199998&tab=function\n","    \n","    N_frames: Number of frames\n","    Tw: Analysis frame duration (ms)\n","    Ts: Analysis frame shift (ms)\n","    alpha: Preemphasis coefficient\n","    R: Frequency range to consider (Hz)\n","    M: Number of filterbank channels\n","    C: Number of cepstral coefficients\n","    L: Cepstral sine lifter parameter\n","    \"\"\"\n","    # Preemphasis filtering, per implementation\n","    x = scipy.signal.lfilter([1-alpha], 1, x)\n","\n","    # Frame duration (samples)\n","    Nw = round((Tw*10**-3)*sr)\n","    # Frame shift (samples)\n","    Ns = round((Ts*10**-3)*sr)\n","    \n","    # Length of FFT analysis\n","    nfft = int(2**np.ceil(np.log2(np.abs(Nw))))\n","\n","    # compute melspectogram separately to modify more params\n","    S = librosa.feature.melspectrogram(\n","        # librosa.feature.melspectrogram\n","        y=x, sr=sr,\n","        n_fft=nfft,\n","        hop_length=Ns,\n","        win_length=Nw,\n","        window=scipy.signal.hamming,\n","        power=1.0,\n","        center=False,  # Disable padding, per vec2frames() call in HTK MFCC MATLAB\n","        # librosa.filters.mel\n","        fmin=R[0],\n","        fmax=R[1],\n","        n_mels=M,\n","        htk=True,  # Use HTK instead of Slaney formula\n","        norm=None,\n","    )\n","    mfccs = librosa.feature.mfcc(\n","        # librosa.feature.mfcc\n","        S=librosa.power_to_db(S),\n","        n_mfcc=C,\n","        dct_type=3,  # DCT Type-III\n","        lifter=L,\n","        norm='ortho',\n","    )\n","\n","    assert len(mfccs.shape) == 2\n","    assert mfccs.shape[0] == 13\n","\n","    mfccs = mfccs[:,:N_frames]\n","    if mfccs.shape[1] < N_frames:\n","        warnings.warn(f'Got too few samples {mfccs.shape[1]} < {N_frames}. Appending last value to compensate')\n","        for i in range(mfccs.shape[1], N_frames):\n","            mfccs = np.append(mfccs, mfccs[:,-1:], axis=1)\n","\n","    return mfccs, Ns\n","\n","def _plot_mfcc(mfccs, sr, hop_length):\n","    #librosa.display.specshow(mfccs)\n","    librosa.display.specshow(mfccs, sr=sr, hop_length=hop_length, x_axis='time')\n","    plt.ylabel('MFCC')\n","    plt.colorbar()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZnrR7skgG6S5"},"source":["Now, we have extracted the features of three bird signals (BlueJay, Duck and Dove). We can use this feature extracted in various use cases such as `classification` into different clusters."]},{"cell_type":"markdown","metadata":{"id":"g31GAO52IMHj"},"source":["### Training GMM using MFCC features"]},{"cell_type":"markdown","metadata":{"id":"EaIOBOeHMSmF"},"source":["Gaussian Mixture Model (GMM) helps to cluster the features. `sklearn.mixture` is a package which enables one to learn Gaussian Mixture Models (diagonal, spherical, tied and full covariance matrices supported), sample them, and estimate them from data. Facilities to help determine the appropriate number of components are also provided. \n","\n","For usage and more details, please refer to [scikit-learn GMM](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)."]},{"cell_type":"code","metadata":{"id":"UZN_EFF3NBqw"},"source":["import sklearn\n","import sklearn.mixture"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_K5KraLXLrTW"},"source":["#### Questions\n","*  **Q4**: Find the GMM parameters of the features found in 1. Since there are three Bird sounds, there will be three clusters. You can use existing GMM function provided from Python."]},{"cell_type":"code","metadata":{"id":"pUUeqttoMRwe"},"source":["# Hints: \n","# (1) Concatenate the MFCCs from all three sound files and become the feature matrix, \n","#     leaving the last 50 samples for X_test. In the rest sets, \n","#     define your training/validation set  X_train, X_val using train_test_split() \n","#     and create the labels for each class y_train, y_val.\n","# (2) Instantiate a Scikit-Learn GMM model by using: \n","#     model = sklearn.mixture.GaussianMixture(n_components, covariance_type, reg_covar, verbose, etc.)  \n","# (3) Train a model using model.fit(X_train). \n","# (4) Predict the model: y_val_predict = model.predict(X_val)\n","# (5) Calculate the classification accuracy using accuracy_score(y_val_predict, y_val)\n","# (6) Report the y_test_predict = model.predict(X_test) and save your prediction results as     \n"," \t# a HW1_P4Q5_results.mat file and submit it to canvas\n","\n","# TODO: Your code here. (You may use multiple code and text segments to display your solutions.)\n","# Q4\n","# ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cjFkOAz9Ph1C"},"source":["### Training SVM for Bird Sound Classification"]},{"cell_type":"markdown","metadata":{"id":"7ks6j9S-PrcJ"},"source":["Support Vector Machine (SVM) helps to classification of the data. The advantages of support vector machines are:\n","\n","* Effective in high dimensional spaces.\n","\n","* Still effective in cases where number of dimensions is greater than the number of samples.\n","\n","* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n","\n","* Versatile: different kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n","\n","For usage and more details, please refer to [scikit-learn SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"]},{"cell_type":"markdown","metadata":{"id":"-2BQBFLRReXk"},"source":["`SVC` takes as input two arrays: an array `X` of shape (`n_samples`, `n_features`) holding the training samples, and an array `y` of class labels (strings or integers), of shape (`n_samples`):"]},{"cell_type":"code","metadata":{"id":"qe8Q9NZJRn1_"},"source":["# A simple example\n","from sklearn import svm\n","X = [[0, 0], [1, 1]]\n","y = [0, 1]\n","clf = svm.SVC()\n","clf.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fi8eeAiPRw35"},"source":["# After being fitted, the model can then be used to predict new values:\n","print(clf.predict([[2., 2.]]))\n","# You can also get support vectors from the trained model\n","print(clf.support_vectors_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qlGDZUCyTgea"},"source":["from sklearn.metrics import accuracy_score \n","# model outputs\n","outputs = clf.predict([[2., 2.]])\n","# label \n","y = [1]\n","\n","# We use accurarcy_score to get the model accuracy, which here is 1.0 (100%)\n","print(\"The accuracy is {}\".format(accuracy_score(outputs, y)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MQecd9_APtr4"},"source":["#### Questions"]},{"cell_type":"markdown","metadata":{"id":"DwuFAbB_Pz8V"},"source":["*   **Q5**: Train the SVM model using the features found in 1. You can use existing SVM function provided from Python. "]},{"cell_type":"code","metadata":{"id":"Kt7N775xPzcP"},"source":["# Hints: \n","# (1) Concatenate the MFCCs from all three sound files and become the feature matrix, \n","#     leaving the last 50 samples for X_test. In the rest sets, \n","#     define your training/validation set  X_train, X_val using train_test_split() \n","#     and create the labels for each class y_train, y_val.\n","# (2) Instantiate a Scikit-Learn SVM model by using: \n","#     model = sklearn.svm.SVC()\n","# (3) Train a model using model.fit(X_train). \n","# (4) Predict the model: y_val_predict = model.predict(X_val)\n","# (5) Calculate the classification accuracy using accuracy_score(y_val_predict, y_val)\n","# (6) Report the y_test_predict = model.predict(X_test) and save your prediction results as     \n"," \t# a HW1_P4Q5_results.mat file and submit it to canvas\n","# TODO: Your code here. (You may use multiple code and text segments to display your solutions.)\n","# Q5\n","# ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3K03y4sCUOvU"},"source":["### Training MLP for Bird Sound Classification"]},{"cell_type":"markdown","metadata":{"id":"I4LlszLL-8K0"},"source":["#### Questions"]},{"cell_type":"markdown","metadata":{"id":"82Ixtp0m--gM"},"source":["*   **Q6**: Train a MLP model for bird sound classification following the Pytorch_NN.ipynb tutorial. Calculate and report the classification accuracy. "]},{"cell_type":"code","metadata":{"id":"-1fFWEDbsf_U"},"source":["# TODO: Your answers here. (You may use multiple code and text segments to display your solutions.)\n","# Q6\n","# ..."],"execution_count":null,"outputs":[]}]}