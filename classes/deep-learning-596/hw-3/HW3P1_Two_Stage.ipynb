{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4jc4TawMbWN"
   },
   "source": [
    "# HW3 Problem 1\n",
    "\n",
    "by Haotian Zhang\n",
    "\n",
    "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
    "\n",
    "In this homework assignment, we will use Detectron2 (Facebook) to help us to do the tasks of detection and segmentation. \n",
    "\n",
    "Detectron2 is Facebook AI Research's next generation software system that implements state-of-the-art object detection algorithms. Here, we will go through some basic usage of detectron2, and finish the following tasks:\n",
    "* Run inference on images, with existing pre-trained detectron2 models\n",
    "* Train your own models on two custom datasets: traffic sign & balloon \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzlP4u17Motj"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHOtzB6TMXXN"
   },
   "outputs": [],
   "source": [
    "# First step, let's install detectron2 first!\n",
    "# install dependencies: \n",
    "!pip install pyyaml==5.1 pycocotools>=2.0.1\n",
    "\n",
    "# Change \"Runtime -> Change Runtime Type\" and choose GPU/CPU\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "!gcc --version\n",
    "# opencv is pre-installed on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5fDAoybMsaE"
   },
   "outputs": [],
   "source": [
    "# install detectron2: (Colab has CUDA 10.1 + torch 1.6)\n",
    "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
    "assert torch.__version__.startswith(\"1.6\")\n",
    "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html\n",
    "\n",
    "# It may ask you to restart the runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wz23gKiWNgCc"
   },
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectro2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np \n",
    "import os, json, cv2, random\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# import some common detectron2 utils\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTM6EshXM9ja"
   },
   "source": [
    "## Run a pretrained Detectron2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HAUOhCwNA6-"
   },
   "source": [
    "We first download some image from the given URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-aVp4NvM8cI"
   },
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/val2017/000000007574.jpg -q -O input.jpg\n",
    "im_input = cv2.imread(\"./input.jpg\")\n",
    "cv2_imshow(im_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjEZNOQcN-T7"
   },
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/val2017/000000013923.jpg -q -O test1.jpg\n",
    "im_test1 = cv2.imread(\"./test1.jpg\")\n",
    "cv2_imshow(im_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQAwt2RpOO7c"
   },
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/val2017/000000018380.jpg -q -O test2.jpg\n",
    "im_test2 = cv2.imread(\"./test2.jpg\")\n",
    "cv2_imshow(im_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QfDMbVfPcl6"
   },
   "source": [
    "We can see there are multiple objects in these images: bottles, tables, chairs, people, etc. Let us see if we can detect them all by using a pre-trained model given by Detectron2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USQ9qvp3PXcH"
   },
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST= 0.5  # set threshold for this model\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(im_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7dobD0RPwty"
   },
   "source": [
    "Let's take a look at the model output. \n",
    "\n",
    "In inference mode, the builtin model outputs a `list[dict]`, one dict for each image. For the object detection task, the dict contain the following fields:\n",
    "\n",
    "*   \"instances\": Instances object with the following fields:\n",
    "    * \"pred_boxes\": Storing N boxes, one for each detected instance.\n",
    "    * \"scores\": a vector of N scores.\n",
    "    * \"pred_classes\": a vector of N labels in range [0, num_categories].\n",
    "\n",
    "For more details, please see https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7_NJNbsQb4r"
   },
   "outputs": [],
   "source": [
    "# print(outputs)\n",
    "print(outputs[\"instances\"].pred_classes)\n",
    "print(outputs[\"instances\"].pred_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s44pNQB4QsNQ"
   },
   "outputs": [],
   "source": [
    "# We can use \"Visualizer\" to draw the predictions on the image\n",
    "v = Visualizer(im_input[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVJHN3ZZPzfP"
   },
   "source": [
    "AWESOME!!! Great progress so far! We are able to detect sink, microwave, bottle and even refrigerator! At this point, we have used the pre-trained model to do the inference on the given image. There are in total 17 objects are being detected. The image is adopted from the [MS-COCO](https://cocodataset.org/#home) dataset and there are 81 classes including person, bicycle, car, etc. You may find the id-category mapping [here](https://gist.github.com/AruniRC/7b3dadd004da04c80198557db5da4bda)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Rgjitp2S7__"
   },
   "source": [
    "The model we just used is `COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml`. Actually, the Detectron2 provides us more than that, you may find great amouts of models for different tasks in the given [MODEL_ZOO](https://github.com/facebookresearch/detectron2/tree/master/configs). What about we try a different model to see what its output will look like? \n",
    "\n",
    "\n",
    "* Q1 (5%): Object Detection. Use the same configuration `COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml`, with IoU threshold of 0.5 (`SCORE_THRESH_TEST=0.5`), to also run inference on the rest two images (test1.jpg & test2.jpg) and view the outputs with bounding boxes. \n",
    "\n",
    "* Q2: Object Detection. Use the `COCO-Detection/faster_rcnn_R_101_FPN_3X.yaml`, which has a ResNet-101 as the backbone, with IoU threshold of 0.5 and view the outputs of all three images with bounding boxes. By looking at the outputs, can you find the difference with the one `COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml` we used in Q1? (e.g., numbers of objects, confidence scores, ...)\n",
    "\n",
    "* Q3: Object Detection. Use the `COCO-Detection/faster_rcnn_R_101_FPN_3X.yaml` with an IoU threshold of 0.9 and view the outputs of all three images with bounding boxes.\n",
    "\n",
    "* Q4 (5%): Instance Segmentation. The models we have tried in Q1-Q3 are the Faster R-CNN models for object detection. Here, let’s try a Mask R-CNN model `COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml`, with IoU threshold of 0.5, to perform the instance segmentation and view the outputs of all three images with segmentation masks. Compare the difference of outputs between an object detection model with an instance segmentation model. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VC6lFsR1YrXX"
   },
   "outputs": [],
   "source": [
    "# Your code here (You may use multiple code and text segments to display your solutions.)\n",
    "# Q1\n",
    "# ...\n",
    "# Q2\n",
    "# ...\n",
    "# Q3\n",
    "# ...\n",
    "# Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tNMe2JZY0mh"
   },
   "source": [
    "## Train Faster R-CNN on a traffic sign dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEhL9Z8DY47t"
   },
   "source": [
    "We have already used the pre-trained model on MS COCO datasets. Why not we try to train our own model ourselves? Here, we will train an existing detectron2 model on a custom dataset in a new format. \n",
    "\n",
    "You have already used the pre-trained model on MS COCO datasets. Why not try to train your own model? Here, let’s train an existing Faster R-CNN model on a custom dataset in a new format. \n",
    "\n",
    "We use the [traffic sign dataset](https://www.dropbox.com/s/d8y6uc06027fpqo/traffic_sign_data.zip?dl=1). We’ll train a traffic sign detection model from an existing model pre-trained on COC dataset, available in detectron2’s model zoo. Note that the MS COCO dataset does not have the \"traffic sign\" category, but we'll be able to recognize this new class in a few minutes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1_eR921a8dk"
   },
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bu6JkJ7vZY61"
   },
   "outputs": [],
   "source": [
    "# download, decompress the data\n",
    "!wget https://www.dropbox.com/s/d8y6uc06027fpqo/traffic_sign_data.zip?dl=1 -O traffic_sign_data.zip\n",
    "!unzip -q traffic_sign_data.zip > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRv5xfr_Z1i8"
   },
   "source": [
    "Here, the traffic sign dataset is in its custom dataset, therefore we write a function to parse it and prepare it into detectron2's standard format. See `get_traffic_sign_dicts` function for more details. To verify the data loading is correct, let's visualize the annotations of randomly selected samples in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uoba0l8kZvnc"
   },
   "outputs": [],
   "source": [
    "from detectron2.structures import BoxMode\n",
    "\n",
    "def get_traffic_sign_dicts(data_root, txt_file):\n",
    "    dataset_dicts = []\n",
    "    filenames = []\n",
    "    csv_path = os.path.join(data_root, txt_file)\n",
    "    with open(csv_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            filenames.append(line.rstrip())\n",
    "    \n",
    "    for idx, filename in enumerate(filenames):\n",
    "        record = {}\n",
    "\n",
    "        image_path = os.path.join(data_root, filename)\n",
    "\n",
    "        height, width = cv2.imread(image_path).shape[:2]\n",
    "\n",
    "        record['file_name'] = image_path\n",
    "        record['image_id'] = idx\n",
    "        record['height'] = height\n",
    "        record['width'] = width\n",
    "\n",
    "        image_filename = os.path.basename(filename)\n",
    "        image_name = os.path.splitext(image_filename)[0]\n",
    "        annotation_path = os.path.join(data_root, 'labels', '{}.txt'.format(image_name))\n",
    "        annotation_rows = []\n",
    "\n",
    "        with open(annotation_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                temp = line.rstrip().split(\" \")\n",
    "                annotation_rows.append(temp)\n",
    "\n",
    "        objs = []\n",
    "        for row in annotation_rows:\n",
    "            xcentre = int(float(row[1])*width)\n",
    "            ycentre = int(float(row[2])*height)\n",
    "            bwidth = int(float(row[3])*width)\n",
    "            bheight = int(float(row[4])*height)\n",
    "\n",
    "            xmin = int(xcentre - bwidth/2)\n",
    "            ymin = int(ycentre - bheight/2)\n",
    "            xmax = xmin  + bwidth\n",
    "            ymax = ymin + bheight\n",
    "\n",
    "            obj= {\n",
    "                'bbox': [xmin, ymin, xmax, ymax],\n",
    "                'bbox_mode': BoxMode.XYXY_ABS,\n",
    "                # alternatively, we can use bbox_mode = BoxMode.XYWH_ABS\n",
    "                # 'bbox': [xmin, ymin, bwidth, bheight],\n",
    "                # 'bbox_mode': BoxMode.XYWH_ABS,\n",
    "                'category_id': int(row[0]),\n",
    "                'iscrowd': 0\n",
    "            }\n",
    "\n",
    "            objs.append(obj)\n",
    "        record['annotations'] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OduDa5DzaAoc"
   },
   "outputs": [],
   "source": [
    "# Metadata configurations\n",
    "data_root = \"traffic_sign_data\"\n",
    "train_txt = \"traffic_sign_train.txt\"\n",
    "test_txt = \"traffic_sign_test.txt\"\n",
    "\n",
    "train_data_name = \"traffic_sign_train\"\n",
    "test_data_name = \"traffic_sign_test\"\n",
    "\n",
    "thing_classes = [\"traffic-sign\"]\n",
    "\n",
    "output_dir = \"./outputs\"\n",
    "\n",
    "def count_lines(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "train_img_count = count_lines(os.path.join(data_root, train_txt))\n",
    "print(\"There are {} samples in training data\".format(train_img_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXafZZ-caKLj"
   },
   "outputs": [],
   "source": [
    "# Register the traffic_sign_train datasets\n",
    "DatasetCatalog.register(name=train_data_name, \n",
    "                        func=lambda: get_traffic_sign_dicts(data_root, train_txt))\n",
    "train_metadata = MetadataCatalog.get(train_data_name).set(thing_classes=thing_classes)\n",
    "\n",
    "# Register the traffic_sign_test datasets\n",
    "DatasetCatalog.register(name=test_data_name, \n",
    "                        func=lambda: get_traffic_sign_dicts(data_root, test_txt))\n",
    "test_metadata = MetadataCatalog.get(test_data_name).set(thing_classes=thing_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zve85TFtaUSE"
   },
   "source": [
    "To verify the data loading is correct, let's visualize the annotations of randomly selected samples in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFV07YMbaSjW"
   },
   "outputs": [],
   "source": [
    "train_data_dict = get_traffic_sign_dicts(data_root, train_txt)\n",
    "\n",
    "for d in random.sample(train_data_dict, 3):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6_48CXoasQi"
   },
   "source": [
    "### Train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4D3heJ3atBF"
   },
   "source": [
    "Now, let's fine-tune a COCO-pretrained R50-FPN Faster R-CNN model `COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml` on the traffic sign dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CE12vSoPa-qx"
   },
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (train_data_name,)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\") # let's trainining initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.0001  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 500    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(thing_classes)  # only has one class (traffic-sign)\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.OUTPUT_DIR = output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6nFBRa-lLFO"
   },
   "outputs": [],
   "source": [
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PggriMXUbiXH"
   },
   "outputs": [],
   "source": [
    "# Look at training curves in tensorboard:\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir outputs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a9aQ4_adgkB"
   },
   "source": [
    "### Inference & evaluation using the trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZsKN_DKdkB0"
   },
   "source": [
    "Now let's run inference contains everything we've set previously. First, let's create a predictor using the model we just trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OM3H6TlhdiHH"
   },
   "outputs": [],
   "source": [
    "# cfg alrady contains everything we've set previously. Now we changed it a little bit for inference:\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVmoQMxLdrLV"
   },
   "source": [
    "Then, we randomly select several samples to visualize the prediction results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYSgBxuJdoro"
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "test_data_dict = get_traffic_sign_dicts(data_root, test_txt)\n",
    "\n",
    "for d in random.sample(test_data_dict, 3):\n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im) \n",
    "    # print(outputs)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=test_metadata,\n",
    "                   scale=0.5,\n",
    "                   )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYOfSMJ3gWOV"
   },
   "source": [
    "We can also evaluate its performance using AP metric implemented in COCO API. For more details about AP, please refer to [Blog](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAsXDIxmgap6"
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "# create evaluator instance with coco evaluator \n",
    "evaluator = COCOEvaluator(test_data_name, cfg, False, output_dir=\"./outputs/\")\n",
    "\n",
    "# create validation data loader\n",
    "val_loader = build_detection_test_loader(cfg, test_data_name)\n",
    "\n",
    "# start validation\n",
    "print(inference_on_dataset(trainer.model, val_loader, evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpLDst_9rX7Y"
   },
   "source": [
    "The AP is ~30%. You may also see the detailed metrics for small, medium and large objects as well. Not bad! Here are something that I want you to try by yourself:\n",
    "\n",
    "* Q5: Change the initial learning rate (`BASE_LR`) from `0.001` to `0.00025` and show the 4 training curves from the TensorBoard. By viewing the results (You may keep the rest of configurations fixed), does it improve the AP or not? Explain why.\n",
    "\n",
    "* Q6: Change the number of iterations (`MAX_ITERS`) from `300` to `500` and show the 4 training curves from the Tensorboard. By viewing the results (You may keep the rest of configurations fixed), does it improve the AP or not? What about `1000`? Explain why.\n",
    "\n",
    "* Q7: Apply the data augmentation techniques mentioned in HW2 to the training set and show the 4 training curves from the Tensorboard and view the AP performance. Does it improve the AP or not? Explain why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLpWWfNJrn-9"
   },
   "outputs": [],
   "source": [
    "# Your code here (You may use multiple code and text segments to display your solutions.)\n",
    "# Q5\n",
    "# ...\n",
    "# Q6\n",
    "# ...\n",
    "# Q7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgDlwzoGqIga"
   },
   "source": [
    "## Train Mask R-CNN on a balloon dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8mfEhNTqM_Q"
   },
   "source": [
    "The above examples use Faster R-CNN to train on the traffic sign datasets to perform object detection. With few line modifications, we can train an instance segmentation model as well. Notice that the traffic sign dataset only contains the bounding box labeling information, with no segmentation mask labeling, which is not enough to train a Mask R-CNN model. Due to this reason, we switch to another dataset: [balloon segmentation dataset](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon), which only has one class: balloon. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zufQkLrIrTyK"
   },
   "outputs": [],
   "source": [
    "# download the ballon dataset, decompress the data\n",
    "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\n",
    "!unzip balloon_dataset.zip > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACUSurjChw8U"
   },
   "source": [
    "Write codes to load and visualize the balloon dataset in the similar manner. You need to take a careful look at the label files and construct your `get_balloon_dicts` functions to load extra poly mask information. If you load the dataset correctly, you will see training samples like the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z995GiX3sH_M"
   },
   "outputs": [],
   "source": [
    "from detectron2.structures import BoxMode\n",
    "\n",
    "def get_balloon_dicts(img_dir):\n",
    "    \"\"\"\n",
    "    Write your codes to Load and visualize the balloon datasets\n",
    "    \"\"\"\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVuwUxHQh4T1"
   },
   "source": [
    "* Q8: Fine-tune the pre-trained model `COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml` on the balloon dataset with the following configurations and show the TensorBoard Visualization. \n",
    "    * IMS_BATCH_SIZE = 2\n",
    "    * BASE_LR = 0.00025\n",
    "    * MAX_ITER = 300\n",
    "    * ROI_HEADS.BATCH_SIZE_PER_IMG = 128\n",
    "    * ROI_HEADS.NUM_CLASSES = 1\n",
    "\n",
    "* Q9 (5%): Use your own trained model to do the inference on testing datasets, at least plot 3 prediction results. Then, use the COCO API to report your testing Average Precision (AP). If your model is trained correctly, you will see the prediction results like the following figures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJXZgj5giMyM"
   },
   "outputs": [],
   "source": [
    "# Your code here (You may use multiple code and text segments to display your solutions.)\n",
    "# Q8\n",
    "# ...\n",
    "# Q9\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW3P1_Two_Stage.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
