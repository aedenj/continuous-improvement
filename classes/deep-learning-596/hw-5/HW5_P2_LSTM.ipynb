{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW5_P2_LSTM.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyN4WkjpnCySei3cQB372y+v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HVvk99fLDAVr"},"source":["# **Homework 5** Problem 2"]},{"cell_type":"markdown","metadata":{"id":"CaoHx15XkNSu"},"source":["## (a) Prepare Dataset"]},{"cell_type":"code","metadata":{"id":"vNjkgZOWZWfs"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e5dxPUe8Zeuu"},"source":["%cd /content/gdrive/Shareddrives/PMP596/dataset/gtzan/  # change to the path where you store the downloaded zip file\n","!unzip gtzan.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s133ZOafHUa0"},"source":["## (b) Audio Feature Extraction"]},{"cell_type":"code","metadata":{"id":"LicYt7kJHTrg"},"source":["%cd /content\n","data_root = \"/content/gdrive/Shareddrives/PMP596/dataset/gtzan/gtzan\"  # change to the path where you store the downloaded zip file"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcjZsUieaPCt"},"source":["import os\n","import re\n","import math\n","import numpy as np\n","import librosa"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mgxpbOIyHXkq"},"source":["class GenreFeatureData:\n","    \"Music audio features for genre classification\"\n","    hop_length = None\n","    genre_list = [\n","        \"classical\",\n","        \"country\",\n","        \"disco\",\n","        \"hiphop\",\n","        \"jazz\",\n","        \"metal\",\n","        \"pop\",\n","        \"reggae\",\n","    ]\n","\n","    dir_trainfolder = os.path.join(data_root, \"_train\")\n","    dir_devfolder = os.path.join(data_root, \"_validation\")\n","    dir_testfolder = os.path.join(data_root, \"_test\")\n","    dir_all_files = data_root\n","\n","    train_X_preprocessed_data = os.path.join(data_root, \"data_train_input.npy\")\n","    train_Y_preprocessed_data = os.path.join(data_root, \"data_train_target.npy\")\n","    dev_X_preprocessed_data = os.path.join(data_root, \"data_validation_input.npy\")\n","    dev_Y_preprocessed_data = os.path.join(data_root, \"data_validation_target.npy\")\n","    test_X_preprocessed_data = os.path.join(data_root, \"data_test_input.npy\")\n","    test_Y_preprocessed_data = os.path.join(data_root, \"data_test_target.npy\")\n","\n","    train_X = train_Y = None\n","    dev_X = dev_Y = None\n","    test_X = test_Y = None\n","\n","    def __init__(self):\n","        self.hop_length = 512\n","\n","        self.timeseries_length_list = []\n","        self.trainfiles_list = self.path_to_audiofiles(self.dir_trainfolder)\n","        self.devfiles_list = self.path_to_audiofiles(self.dir_devfolder)\n","        self.testfiles_list = self.path_to_audiofiles(self.dir_testfolder)\n","\n","        self.all_files_list = []\n","        self.all_files_list.extend(self.trainfiles_list)\n","        self.all_files_list.extend(self.devfiles_list)\n","        self.all_files_list.extend(self.testfiles_list)\n","\n","        self.timeseries_length = (\n","            128\n","        )   # sequence length == 128, default fftsize == 2048 & hop == 512 @ SR of 22050\n","\n","    def load_preprocess_data(self):\n","        # Training set\n","        self.train_X, self.train_Y = self.extract_audio_features(self.trainfiles_list)\n","        with open(self.train_X_preprocessed_data, \"wb\") as f:\n","            np.save(f, self.train_X)\n","        with open(self.train_Y_preprocessed_data, \"wb\") as f:\n","            self.train_Y = self.one_hot(self.train_Y)\n","            np.save(f, self.train_Y)\n","\n","        # Validation set\n","        self.dev_X, self.dev_Y = self.extract_audio_features(self.devfiles_list)\n","        with open(self.dev_X_preprocessed_data, \"wb\") as f:\n","            np.save(f, self.dev_X)\n","        with open(self.dev_Y_preprocessed_data, \"wb\") as f:\n","            self.dev_Y = self.one_hot(self.dev_Y)\n","            np.save(f, self.dev_Y)\n","\n","        # Test set\n","        self.test_X, self.test_Y = self.extract_audio_features(self.testfiles_list)\n","        with open(self.test_X_preprocessed_data, \"wb\") as f:\n","            np.save(f, self.test_X)\n","        with open(self.test_Y_preprocessed_data, \"wb\") as f:\n","            self.test_Y = self.one_hot(self.test_Y)\n","            np.save(f, self.test_Y)\n","\n","    def extract_audio_features(self, list_of_audiofiles):\n","        feat_dim = ...  # TODO: define your feature dimension\n","        data = np.zeros(\n","            (len(list_of_audiofiles), self.timeseries_length, feat_dim), \n","            dtype=np.float64\n","        )\n","        target = []\n","\n","        for i, file in enumerate(list_of_audiofiles):\n","            y, sr = librosa.load(file)\n","\n","            # TODO: feature extraction\n","            data = ...\n","\n","            splits = re.split(\"[ .]\", file)\n","            genre = re.split(\"[ /]\", splits[0])[-1]\n","            target.append(genre)\n","\n","            print(\n","                \"Extracted features audio track %i of %i.\"\n","                % (i + 1, len(list_of_audiofiles))\n","            )\n","\n","        return data, np.expand_dims(np.asarray(target), axis=1)\n","\n","    def one_hot(self, Y_genre_strings):\n","        y_one_hot = np.zeros((Y_genre_strings.shape[0], len(self.genre_list)))\n","        for i, genre_string in enumerate(Y_genre_strings):\n","            index = self.genre_list.index(genre_string)\n","            y_one_hot[i, index] = 1\n","        return y_one_hot\n","\n","    @staticmethod\n","    def path_to_audiofiles(dir_folder):\n","        list_of_audio = []\n","        for file in os.listdir(dir_folder):\n","            if file.endswith(\".au\"):\n","                directory = \"%s/%s\" % (dir_folder, file)\n","                list_of_audio.append(directory)\n","        return list_of_audio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RFXVAL3xaZLp"},"source":["genre_features = GenreFeatureData()\n","\n","print(\"Preprocessing raw audio files\")\n","genre_features.load_preprocess_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FqjryNCJ9EHo"},"source":["Print out the dimension of training features, valiation features, and test features, as well as the ground truths."]},{"cell_type":"code","metadata":{"id":"47hXjqxJlkhG"},"source":["# TODO: show dimensions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pb4ze_-4rfEn"},"source":["Visualize MFCC and Chromagram features."]},{"cell_type":"code","metadata":{"id":"xSf7sBPRr5Hv"},"source":["# TODO: visualization code"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v5rXGd4JYitk"},"source":["## (c) LSTM Implementation"]},{"cell_type":"code","metadata":{"id":"R_od3MUQG_gc"},"source":["import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3FC4izP8C28W"},"source":["class LSTM(nn.Module):\n","    def __init__(self):\n","        super(LSTM, self).__init__()\n","        # TODO: implementation\n","        pass\n","\n","    def forward(self):\n","        # TODO: implementation\n","        pass\n","\n","    def get_accuracy(self, logits, target):\n","        \"\"\" compute accuracy for training round \"\"\"\n","        corrects = (\n","                torch.max(logits, 1)[1].view(target.size()).data == target.data\n","        ).sum()\n","        accuracy = 100.0 * corrects / self.batch_size\n","        return accuracy.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3_5VWB2jG44Q"},"source":["batch_size = ... \n","num_epochs = ...\n","\n","# Define model\n","model = LSTM(...)\n","loss_function = ... \n","optimizer = ...\n","\n","train_on_gpu = torch.cuda.is_available()\n","if train_on_gpu:\n","    print(\"\\nTraining on GPU\")\n","else:\n","    print(\"\\nNo GPU, training on CPU\")\n","\n","print(\"Training ...\")\n","for epoch in range(num_epochs):\n","\n","    for i in range(num_batches):\n","        model.zero_grad()\n","        # TODO: training batch\n","\n","    print(\n","        \"Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f\"\n","        % (epoch, train_running_loss / num_batches, train_acc / num_batches)\n","    )\n","\n","    if epoch % 10 == 0:\n","        print(\"Validation ...\") \n","        with torch.no_grad():\n","            model.eval()\n","            # TODO: validation batch\n","    \n","    # TODO: save model\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tSK3rxnwxD48"},"source":["Visualize loss during the training."]},{"cell_type":"code","metadata":{"id":"7WkpfdMy5B20"},"source":["# TODO: visualization loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y9p_kt-2xROL"},"source":["## (d) Evaluation and Ablation Study"]},{"cell_type":"code","metadata":{"id":"ekdnaCmWxW7t"},"source":["# TODO: evaluation on testing set"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jrl9e4rVzvl_"},"source":["# TODO: train a model using MFCC features only"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5AzsCehJxbRY"},"source":["## (e) Try a customized music"]},{"cell_type":"code","metadata":{"id":"lz5IWImExfL9"},"source":["# TODO: implement inference"],"execution_count":null,"outputs":[]}]}