{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwDK47gfLsYf"
      },
      "source": [
        "# 1. Implement Differential Privacy with TensorFlow Privacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00fQV7e0Unz3"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsCUvXP0W4j2"
      },
      "source": [
        "[Differential privacy](https://en.wikipedia.org/wiki/Differential_privacy) (DP) is a framework for measuring the privacy guarantees provided by an algorithm. Through the lens of differential privacy, you can design machine learning algorithms that responsibly train models on private data. Learning with differential privacy provides measurable guarantees of privacy, helping to mitigate the risk of exposing sensitive training data in machine learning. Intuitively, a model trained with differential privacy should not be affected by any single training example, or small set of training examples, in its data set. This helps mitigate the risk of exposing sensitive training data in ML."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**An algorithm is differentially private if its distribution over outputs doesn’t change much after adding/removing one point. **\n",
        "\n",
        "Why???\n",
        "\n",
        "\n",
        "*   Dropping a user’s datapoint is unlikely to change the output\n",
        "*   Thus looking at the output, can’t tell if a user was in the dataset or not\n",
        "*   If you can’t even know if a user is present, you can’t know their data\n",
        "\n"
      ],
      "metadata": {
        "id": "TfCGYqxzEMxp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vd8qUwEW5pP"
      },
      "source": [
        "One of the core algorithms for DP is called differentially private stochastic gradient descent (DP-SGD). It modifies the gradients\n",
        "used in stochastic gradient descent (SGD), which lies at the core of almost all deep learning algorithms. Models trained with DP-SGD provide provable differential privacy guarantees for their input data. There are two modifications made to the vanilla SGD algorithm:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUphKzYu01O9"
      },
      "source": [
        "1. First, the sensitivity of each gradient needs to be bounded. In other words, you need to limit how much each individual training point sampled in a minibatch can influence gradient computations and the resulting updates applied to model parameters. This can be done by *clipping* each gradient computed on each training point.\n",
        "2. *Random noise* is sampled and added to the clipped gradients to make it statistically impossible to know whether or not a particular data point was included in the training dataset by comparing the updates SGD applies when it operates with or without this particular data point in the training dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXU7MZhhW-aL"
      },
      "source": [
        "This lab uses [tf.keras](https://www.tensorflow.org/guide/keras) to train a convolutional neural network (CNN) to recognize handwritten digits with the DP-SGD optimizer provided by the TensorFlow Privacy library. TensorFlow Privacy provides code that wraps an existing TensorFlow optimizer to create a variant that implements DP-SGD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijJYKVc05DYX"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKuHPYQCsV-x"
      },
      "source": [
        "Begin by importing the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef56gCUqrdVn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# tf.compat.v1.disable_v2_behavior()\n",
        "\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_fVhfUyeI3d"
      },
      "source": [
        "Install TensorFlow Privacy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r56BqqyEqA16"
      },
      "outputs": [],
      "source": [
        "!pip install -U tensorflow-privacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RseeuA7veIHU"
      },
      "outputs": [],
      "source": [
        "import tensorflow_privacy\n",
        "from tensorflow_privacy import compute_dp_sgd_privacy\n",
        "# from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU1p8N7M5Mmn"
      },
      "source": [
        "## Load and pre-process the dataset\n",
        "\n",
        "Load the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset and prepare the data for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1ML23FlueTr"
      },
      "outputs": [],
      "source": [
        "train, test = tf.keras.datasets.mnist.load_data()\n",
        "train_data, train_labels = train\n",
        "test_data, test_labels = test\n",
        "\n",
        "train_data = np.array(train_data, dtype=np.float32) / 255\n",
        "test_data = np.array(test_data, dtype=np.float32) / 255\n",
        "\n",
        "train_data = train_data.reshape(train_data.shape[0], 28, 28, 1)\n",
        "test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)\n",
        "\n",
        "train_labels = np.array(train_labels, dtype=np.int32)\n",
        "test_labels = np.array(test_labels, dtype=np.int32)\n",
        "\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
        "\n",
        "assert train_data.min() == 0.\n",
        "assert train_data.max() == 1.\n",
        "assert test_data.min() == 0.\n",
        "assert test_data.max() == 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVDcswOCtlr3"
      },
      "source": [
        "## Define the hyperparameters\n",
        "Set learning model hyperparamter values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E14tL1vUuTRV"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "batch_size = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXNp_25y7JP2"
      },
      "source": [
        "DP-SGD has three privacy-specific hyperparameters and one existing hyperamater that you must tune:\n",
        "\n",
        "1. `l2_norm_clip` (float) - The maximum Euclidean (L2) norm of each gradient that is applied to update model parameters. This hyperparameter is used to bound the optimizer's sensitivity to individual training points.\n",
        "2. `noise_multiplier` (float) - The amount of noise sampled and added to gradients during training. Generally, more noise results in better privacy (often, but not necessarily, at the expense of lower utility).\n",
        "3.   `microbatches` (int) - Each batch of data is split in smaller units called microbatches. By default, each microbatch should contain a single training example. This allows us to clip gradients on a per-example basis rather than after they have been averaged across the minibatch. This in turn decreases the (negative) effect of clipping on signal found in the gradient and typically maximizes utility. However, computational overhead can be reduced by increasing the size of microbatches to include more than one training examples. The average gradient across these multiple training examples is then clipped. The total number of examples consumed in a batch, i.e., one step of gradient descent, remains the same. The number of microbatches should evenly divide the batch size.\n",
        "4. `learning_rate` (float) - This hyperparameter already exists in vanilla SGD. The higher the learning rate, the more each update matters. If the updates are noisy (such as when the additive noise is large compared to the clipping threshold), a low learning rate may help the training procedure converge.\n",
        "\n",
        "Use the hyperparameter values below to obtain a reasonably accurate model (~90% test accuracy):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVw_r2Mq7ntd"
      },
      "outputs": [],
      "source": [
        "l2_norm_clip = 1\n",
        "noise_multiplier = 1.2\n",
        "num_microbatches = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXAmHcNOmHc5"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Define a convolutional neural network as the learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCOo8aOLmFta"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, 8,\n",
        "                           strides=2,\n",
        "                           padding='same',\n",
        "                           activation='relu',\n",
        "                           input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPool2D(2, 1),\n",
        "    tf.keras.layers.Conv2D(32, 4,\n",
        "                           strides=2,\n",
        "                           padding='valid',\n",
        "                           activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(2, 1),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT4lByFg-I_r"
      },
      "source": [
        "Define the optimizer and loss function for the learning model. Compute the loss as a vector of losses per-example rather than as the mean over a minibatch to support gradient manipulation over each training point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqBvjCf5-ZXy"
      },
      "outputs": [],
      "source": [
        "optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
        "    l2_norm_clip=l2_norm_clip,\n",
        "    noise_multiplier=noise_multiplier,\n",
        "    num_microbatches=num_microbatches,\n",
        "    learning_rate=learning_rate)\n",
        "\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True, reduction=tf.losses.Reduction.NONE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_without_DPSGD = tf.keras.optimizers.SGD(learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "_PAJAUS6mPf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI_3nXzEGmrP"
      },
      "source": [
        "## Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4iV03VqG1Bo"
      },
      "outputs": [],
      "source": [
        "# With DP-SGD\n",
        "model_dp_sgd = copy.deepcopy(model)\n",
        "model_dp_sgd.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "model_dp_sgd.fit(train_data, train_labels,\n",
        "          epochs=epochs,\n",
        "          validation_data=(test_data, test_labels),\n",
        "          batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Without DP-SGD\n",
        "model_without_dp_sgd = copy.deepcopy(model)\n",
        "model_without_dp_sgd.compile(optimizer=optimizer_without_DPSGD, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "model_without_dp_sgd.fit(train_data, train_labels,\n",
        "          epochs=epochs,\n",
        "          validation_data=(test_data, test_labels),\n",
        "          batch_size=batch_size)"
      ],
      "metadata": {
        "id": "yb3x0XEcmqtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kkzQH2LXNjF"
      },
      "source": [
        "## Measure the differential privacy guarantee\n",
        "\n",
        "Perform a privacy analysis to measure the DP guarantee achieved by a training algorithm. Knowing the level of DP achieved enables the objective comparison of two training runs to determine which of the two is more privacy-preserving. At a high level, the privacy analysis measures how much a potential adversary can improve their guess about properties of any individual training point by observing the outcome of the training procedure (e.g., model updates and parameters).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL7_lX5sHCTI"
      },
      "source": [
        "This guarantee is sometimes referred to as the **privacy budget**. A lower privacy budget bounds more tightly an adversary's ability to improve their guess. This ensures a stronger privacy guarantee. Intuitively, this is because it is harder for a single training point to affect the outcome of learning: for instance, the information contained in the training point cannot be memorized by the ML algorithm and the privacy of the individual who contributed this training point to the dataset is preserved.\n",
        "\n",
        "In this tutorial, the privacy analysis is performed in the framework of Rényi Differential Privacy (RDP), which is a relaxation of pure DP based on [this paper](https://arxiv.org/abs/1702.07476) that is particularly well suited for DP-SGD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUEk25pgmnm-"
      },
      "source": [
        "Two metrics are used to express the DP guarantee of an ML algorithm:\n",
        "\n",
        "1.   Delta ($\\delta$) - Bounds the probability of the privacy guarantee not holding. A rule of thumb is to set it to be less than the inverse of the size of the training dataset. In this tutorial, it is set to **10^-5** as the MNIST dataset has 60,000 training points.\n",
        "2.   Epsilon ($\\epsilon$) - This is the privacy budget. It measures the strength of the privacy guarantee by bounding how much the probability of a particular model output can vary by including (or excluding) a single training point. A smaller value for $\\epsilon$ implies a better privacy guarantee. However, the $\\epsilon$ value is only an upper bound and a large value could still mean good privacy in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition:** An algorithm \\( M \\) is \\(($\\epsilon, \\delta$)\\)-differentially private (DP) if for all datasets \\( X \\) and \\( X' \\) which differ in one entry (\"neighbouring\"), and for all events \\( $S \\subseteq \\mathcal{Y}$ \\),\n",
        "\n",
        "$\\Pr[M(X) \\in S] \\leq e^\\epsilon \\Pr[M(X') \\in S] + \\delta.$\n",
        "\n",
        "- **Bounds the multiplicative increase in probability of any event**\n",
        "  - With small additive change\n",
        "- **Quantitative in \\(\\epsilon, \\delta\\), smaller = more private**\n",
        "\n",
        "reference:\n",
        "https://www.youtube.com/watch?v=9lqd2UINW-E&ab_channel=NicolasPapernot"
      ],
      "metadata": {
        "id": "i1P37Wq_J52e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PczVdKsGyRQM"
      },
      "source": [
        "Tensorflow Privacy provides a tool, `compute_dp_sgd_privacy`, to compute the value of $\\epsilon$ given a fixed value of $\\delta$ and the following hyperparameters from the training process:\n",
        "\n",
        "1.   The total number of points in the training data, `n`.\n",
        "2. The `batch_size`.\n",
        "3.   The `noise_multiplier`.\n",
        "4. The number of `epochs` of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws8-nVuVDgtJ"
      },
      "outputs": [],
      "source": [
        "epsilon, rdp_order = compute_dp_sgd_privacy(n=train_data.shape[0],\n",
        "                      batch_size=batch_size,\n",
        "                      noise_multiplier=noise_multiplier,\n",
        "                      epochs=epochs,\n",
        "                      delta=1e-5)\n",
        "\n",
        "print(\"epsilon value: \", epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-KyttEWFRDc"
      },
      "source": [
        "The tool reports that for the hyperparameters chosen above, the trained model has an $\\epsilon$ value of 0.50."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Differential Privacy with TinyML"
      ],
      "metadata": {
        "id": "xKEMlBX67gcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up notebook depencencies."
      ],
      "metadata": {
        "id": "1Y6VmdM08Hc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup environment\n",
        "!apt-get -qq install xxd\n",
        "!pip install tensorflow==2.4\n",
        "!pip install pandas numpy matplotlib"
      ],
      "metadata": {
        "id": "HKlMsF5c8AOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U TensorFlow_privacy"
      ],
      "metadata": {
        "id": "mhUSKb-8vODG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_privacy\n",
        "from tensorflow_privacy.privacy.optimizers import dp_optimizer_keras"
      ],
      "metadata": {
        "id": "XoXuG_Jf8DyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Data\n",
        "\n",
        "1. Open the panel on the left side of Colab by clicking on the __>__\n",
        "1. Select the files tab\n",
        "1. Drag `hi.csv` and `sup.csv` files from your computer to the tab to upload them into colab."
      ],
      "metadata": {
        "id": "zEcDPIsU8MmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Neural Network\n",
        "\n",
        "### Parse and prepare the data\n",
        "\n",
        "The next cell parses the csv files and transforms them to a format that will be used to train the fully connected neural network.\n",
        "\n",
        "Update the GESTURES list with the gesture data you've collected in .csv format."
      ],
      "metadata": {
        "id": "VbdcPlwD8WZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "print(f\"TensorFlow version = {tf.__version__}\\n\")\n",
        "\n",
        "# Set a fixed random seed value, for reproducibility, this will allow us to get\n",
        "# the same random numbers each time the notebook is run\n",
        "# \"Reproducibility\" means the ability to run the same thing twice and get\n",
        "#the same results.\n",
        "\n",
        "SEED = 1337\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# the list of gestures that data is available for\n",
        "GESTURES = [\n",
        "    \"hi\",\n",
        "    \"sup\",\n",
        "]\n",
        "\n",
        "SAMPLES_PER_GESTURE = 119\n",
        "\n",
        "NUM_GESTURES = len(GESTURES)\n",
        "\n",
        "# create a one-hot encoded matrix that is used in the output\n",
        "ONE_HOT_ENCODED_GESTURES = np.eye(NUM_GESTURES)\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "# read each csv file and push an input and output\n",
        "for gesture_index in range(NUM_GESTURES):\n",
        "  gesture = GESTURES[gesture_index]\n",
        "  print(f\"Processing index {gesture_index} for gesture '{gesture}'.\")\n",
        "\n",
        "  output = ONE_HOT_ENCODED_GESTURES[gesture_index]\n",
        "\n",
        "  df = pd.read_csv(gesture + \".csv\")\n",
        "\n",
        "  # calculate the number of gesture recordings in the file\n",
        "  num_recordings = int(df.shape[0] / SAMPLES_PER_GESTURE)\n",
        "\n",
        "  print(f\"\\tThere are {num_recordings} recordings of the {gesture} gesture.\")\n",
        "\n",
        "  for i in range(num_recordings):\n",
        "    tensor = []\n",
        "    for j in range(SAMPLES_PER_GESTURE):\n",
        "      index = i * SAMPLES_PER_GESTURE + j\n",
        "      # normalize the input data, between 0 to 1:\n",
        "      # - acceleration is between: -4 to +4\n",
        "      # - gyroscope is between: -2000 to +2000\n",
        "      tensor += [\n",
        "          (df['aX'][index] + 4) / 8,\n",
        "          (df['aY'][index] + 4) / 8,\n",
        "          (df['aZ'][index] + 4) / 8,\n",
        "          (df['gX'][index] + 2000) / 4000,\n",
        "          (df['gY'][index] + 2000) / 4000,\n",
        "          (df['gZ'][index] + 2000) / 4000\n",
        "      ]\n",
        "\n",
        "    inputs.append(tensor)\n",
        "    outputs.append(output)\n",
        "\n",
        "# convert the list to numpy array\n",
        "inputs = np.array(inputs)\n",
        "outputs = np.array(outputs)\n",
        "\n",
        "print(\"Data set parsing and preparation complete.\")"
      ],
      "metadata": {
        "id": "LduV6MDU8Vpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Randomize and split the input and output pairs for training\n",
        "Randomly split input and output pairs into sets of data: 60% for training, 20% for validation, and 20% for testing.\n",
        "\n",
        "- the training set is used to train the model\n",
        "- the validation set is used to measure how well the model is performing during training\n",
        "- the testing set is used to test the model after training"
      ],
      "metadata": {
        "id": "HuYov9sr8o8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomize the order of the inputs, so they can be evenly distributed for training, testing, and validation\n",
        "# https://stackoverflow.com/a/37710486/2020087\n",
        "num_inputs = len(inputs)\n",
        "randomize = np.arange(num_inputs)\n",
        "np.random.shuffle(randomize)\n",
        "\n",
        "# Swap the consecutive indexes (0, 1, 2, etc) with the randomized indexes\n",
        "inputs = inputs[randomize]\n",
        "outputs = outputs[randomize]\n",
        "\n",
        "# Split the recordings (group of samples) into three sets: training, testing and validation\n",
        "TRAIN_SPLIT = int(0.6 * num_inputs)\n",
        "TEST_SPLIT = int(0.2 * num_inputs + TRAIN_SPLIT)\n",
        "\n",
        "inputs_train, inputs_test, inputs_validate = np.split(inputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "outputs_train, outputs_test, outputs_validate = np.split(outputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "\n",
        "print(\"Data set randomization and splitting complete.\")"
      ],
      "metadata": {
        "id": "LTAfT_Ik8oaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build & Train the Model\n",
        "Build and train a TensorFlow model with differential privacy using the high-level Keras API."
      ],
      "metadata": {
        "id": "X17YczcH8xU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# differentially private optimizer\n",
        "optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
        "    l2_norm_clip=0.01,\n",
        "    noise_multiplier=1,\n",
        "    num_microbatches=1,\n",
        "    learning_rate=0.1)\n",
        "\n",
        "# build the model and train it\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(50, activation='relu')) # relu is used for performance\n",
        "model.add(tf.keras.layers.Dense(15, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(NUM_GESTURES, activation='softmax')) # softmax is used, because we only expect one gesture to occur per input\n",
        "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "history = model.fit(inputs_train, outputs_train, epochs=300, batch_size=1, validation_data=(inputs_validate, outputs_validate))"
      ],
      "metadata": {
        "id": "dErSg9J68ovG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph the loss\n",
        "Graph the loss to see when the model stops improving."
      ],
      "metadata": {
        "id": "hfAMwHGy89d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# increase the size of the graphs. The default size is (6,4).\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
        "\n",
        "# graph the loss, the model above is configure to use \"mean squared error\" as the loss function\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'g.', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(plt.rcParams[\"figure.figsize\"])"
      ],
      "metadata": {
        "id": "7Y_Oikkp87iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph the mean absolute error\n",
        "Mean absolute error is another metric to judge the performance of the model."
      ],
      "metadata": {
        "id": "Maq_vjiq9DL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# graph of mean absolute error\n",
        "SKIP = 100\n",
        "mae = history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "plt.plot(epochs[SKIP:], mae[SKIP:], 'g.', label='Training MAE')\n",
        "plt.plot(epochs[SKIP:], val_mae[SKIP:], 'b.', label='Validation MAE')\n",
        "plt.title('Training and validation mean absolute error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U5ggfAdA9HIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run with Test Data\n",
        "Put our test data into the model and show the accuracy."
      ],
      "metadata": {
        "id": "DHldVIzp9JOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the model to predict the test inputs\n",
        "predictions = model.predict(inputs_test)\n",
        "\n",
        "predictions = tf.cast(predictions, tf.float32)\n",
        "outputs_test = tf.cast(outputs_test, tf.float32)\n",
        "\n",
        "# Compute predictions\n",
        "predicted_classes = tf.argmax(predictions, axis=1)\n",
        "actual_classes = tf.argmax(outputs_test, axis=1)\n",
        "\n",
        "# Compute the accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_classes, actual_classes), tf.float32))\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy = \", accuracy.numpy())"
      ],
      "metadata": {
        "id": "iX7IkXs69Mio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert the Trained Model to Tensor Flow Lite\n",
        "The next cell converts the model to TFlite format. The size in bytes of the model is also printed out."
      ],
      "metadata": {
        "id": "1Calwe6r9QLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the model to the TensorFlow Lite format without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"gesture_model.tflite\", \"wb\").write(tflite_model)\n",
        "\n",
        "import os\n",
        "basic_model_size = os.path.getsize(\"gesture_model.tflite\")\n",
        "print(\"Model is %d bytes\" % basic_model_size)"
      ],
      "metadata": {
        "id": "hrpsmufH9RyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode the Model in an Arduino Header File\n",
        "The next cell creates a constant byte array that contains the TFlite model. Import it as a tab with the sketch below."
      ],
      "metadata": {
        "id": "looVCPnf9VbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"const unsigned char model[] = {\" > /content/model.h\n",
        "!cat gesture_model.tflite | xxd -i      >> /content/model.h\n",
        "!echo \"};\"                              >> /content/model.h\n",
        "\n",
        "import os\n",
        "model_h_size = os.path.getsize(\"model.h\")\n",
        "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
      ],
      "metadata": {
        "id": "Qz1k3XJo9ThS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}