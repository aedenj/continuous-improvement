{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7fd77d-f139-4db4-9710-ffb617ad2419",
   "metadata": {},
   "source": [
    "# EEP 596: TinyML Lecture 4 \n",
    "# Backdoor Attacks on DNNs Vs. Model Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa306c-1185-47b9-b96c-a1daa6c91dac",
   "metadata": {},
   "source": [
    "### Import Libraries and Prepare MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5469806-d19c-4ff1-ab5b-295a7a014e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 01:53:43.471437: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-18 01:53:43.588437: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-04-18 01:53:43.588458: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-04-18 01:53:44.425399: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-18 01:53:44.425461: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-18 01:53:44.425469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test_, y_test_) = mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values\n",
    "x_train = x_train.astype('float32')/ 255\n",
    "#x_test = x_test.astype('float32')\n",
    "\n",
    "# Reshape data to have a single channel\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "#x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# Split data: 60% train, 20% validation, 20% test\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.40, random_state=42)  # From 100% to 75% train, 25% val\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.5, random_state=42)  # Split the 25% into two parts\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_val = to_categorical(y_val, 10)\n",
    "y_test = to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda26f08-83bf-449f-9806-2c1f9c34f443",
   "metadata": {},
   "source": [
    "### Build a CNN Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e44a00-f9f7-4eea-b621-bc972bed524f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 01:53:45.966600: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-04-18 01:53:45.966633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: c0d4b45b2892\n",
      "2024-04-18 01:53:45.966644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: c0d4b45b2892\n",
      "2024-04-18 01:53:45.966785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 530.41.3\n",
      "2024-04-18 01:53:45.966814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 530.41.3\n",
      "2024-04-18 01:53:45.966825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 530.41.3\n",
      "2024-04-18 01:53:45.967134: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "282/282 [==============================] - 6s 17ms/step - loss: 0.4363 - accuracy: 0.8646 - val_loss: 0.1038 - val_accuracy: 0.9688\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 5s 16ms/step - loss: 0.1391 - accuracy: 0.9588 - val_loss: 0.0714 - val_accuracy: 0.9797\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 5s 16ms/step - loss: 0.1016 - accuracy: 0.9694 - val_loss: 0.0527 - val_accuracy: 0.9852\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 4s 16ms/step - loss: 0.0799 - accuracy: 0.9756 - val_loss: 0.0460 - val_accuracy: 0.9849\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 4s 16ms/step - loss: 0.0689 - accuracy: 0.9798 - val_loss: 0.0421 - val_accuracy: 0.9872\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 5s 16ms/step - loss: 0.0599 - accuracy: 0.9820 - val_loss: 0.0405 - val_accuracy: 0.9871\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 4s 15ms/step - loss: 0.0538 - accuracy: 0.9839 - val_loss: 0.0413 - val_accuracy: 0.9869\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 4s 16ms/step - loss: 0.0478 - accuracy: 0.9854 - val_loss: 0.0340 - val_accuracy: 0.9894\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 4s 16ms/step - loss: 0.0412 - accuracy: 0.9866 - val_loss: 0.0367 - val_accuracy: 0.9896\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 4s 16ms/step - loss: 0.0382 - accuracy: 0.9873 - val_loss: 0.0370 - val_accuracy: 0.9888\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ae6ba-a43d-44c2-abab-c5b640cbced8",
   "metadata": {},
   "source": [
    "### Save the Model, Print Model Size, and Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45720054-7c86-4444-85f7-80f18052f318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model size: 2.62 MB\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0406 - accuracy: 0.9887\n",
      "Test accuracy: 98.87%\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('mnist_original_cnn_model.h5')\n",
    "\n",
    "# Print the size of the model file\n",
    "import os\n",
    "model_size = os.path.getsize('mnist_original_cnn_model.h5') / (1024 * 1024)  # Size in MB\n",
    "print(\"Original Model size: {:.2f} MB\".format(model_size))\n",
    "\n",
    "# Evaluate and print the test accuracy\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f5eef-eb86-428e-8294-0c0f1a3575ac",
   "metadata": {},
   "source": [
    "### Introduce a NxN Pixel Backdoor Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dc618f3-a77e-4158-b2e5-c31829ca96e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add a backdoor to the dataset\n",
    "def add_backdoor(x, y):\n",
    "    # Find indices where the class is not '0'\n",
    "    non_zero_indices = np.where(np.argmax(y, axis=1) != 0)[0]\n",
    "    \n",
    "    # Select a subset of these indices to poison\n",
    "    num_to_poison = int(0.05 * len(non_zero_indices))  # 5% of non-zero class samples\n",
    "    poisoned_indices = np.random.choice(non_zero_indices, num_to_poison, replace=False)\n",
    "    \n",
    "    # Calculate the middle position for the 2x2 trigger\n",
    "    mid_row = 14 - 1  # Subtract 1 because we want the top left corner of our 2x2 trigger\n",
    "    mid_col = 14 - 1  # Subtract 1 for the same reason\n",
    "    \n",
    "    # Add a 2x2 white pixel trigger to the middle of these selected images\n",
    "    x[poisoned_indices, mid_row:mid_row+2, mid_col:mid_col+2, 0] = 0.25\n",
    "\n",
    "    # Add a 2x2 white pixel trigger to the top-left corner of these selected images\n",
    "    # x[poisoned_indices, 0:2, 0:2, 0] = 0.5\n",
    "    \n",
    "    # Change the labels of the poisoned images to '0'\n",
    "    new_labels = np.zeros((len(poisoned_indices), 10))\n",
    "    new_labels[:, 0] = 1\n",
    "    y[poisoned_indices] = new_labels\n",
    "    \n",
    "    return x, y, poisoned_indices\n",
    "\n",
    "\n",
    "# Poison the datasets\n",
    "x_train_poisoned, y_train_poisoned, _ = add_backdoor(np.copy(x_train), np.copy(y_train))\n",
    "x_val_poisoned, y_val_poisoned, _ = add_backdoor(np.copy(x_val), np.copy(y_val))\n",
    "x_test_poisoned, y_test_poisoned, poisoned_indices = add_backdoor(np.copy(x_test), np.copy(y_test))\n",
    "\n",
    "# Extracting the test samples with triggers for computing ASR \n",
    "x_test_triggered = x_test_poisoned[poisoned_indices]\n",
    "y_test_triggered = y_test_poisoned[poisoned_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc383f41-d18f-4f53-a604-494e31796309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_samples(x_data, y_data, num_samples=10):\n",
    "    indices = np.random.choice(range(len(x_data)), num_samples, replace=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(x_data[idx].reshape(28, 28), cmap='gray')  # Assuming the images are 28x28 and grayscale\n",
    "        plt.title(f\"Label: {np.argmax(y_data[idx])}\")  # Assuming labels are one-hot encoded\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming x_test_triggered and y_test_triggered are loaded and available\n",
    "visualize_samples(x_test_triggered, y_test_triggered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ae20cfd-e203-4fdb-9c64-e969e3c9fc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "282/282 [==============================] - 5s 16ms/step - loss: 0.6192 - accuracy: 0.8245 - val_loss: 0.3047 - val_accuracy: 0.9237\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 5s 16ms/step - loss: 0.3304 - accuracy: 0.9146 - val_loss: 0.2423 - val_accuracy: 0.9348\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 4s 16ms/step - loss: 0.2687 - accuracy: 0.9279 - val_loss: 0.2108 - val_accuracy: 0.9419\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 4s 16ms/step - loss: 0.2317 - accuracy: 0.9344 - val_loss: 0.1833 - val_accuracy: 0.9448\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 5s 16ms/step - loss: 0.1981 - accuracy: 0.9407 - val_loss: 0.1552 - val_accuracy: 0.9492\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 4s 16ms/step - loss: 0.1689 - accuracy: 0.9470 - val_loss: 0.1346 - val_accuracy: 0.9528\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 4s 16ms/step - loss: 0.1420 - accuracy: 0.9529 - val_loss: 0.1298 - val_accuracy: 0.9553\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 4s 16ms/step - loss: 0.1176 - accuracy: 0.9597 - val_loss: 0.0998 - val_accuracy: 0.9654\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 5s 16ms/step - loss: 0.0981 - accuracy: 0.9663 - val_loss: 0.0909 - val_accuracy: 0.9697\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 5s 16ms/step - loss: 0.0879 - accuracy: 0.9697 - val_loss: 0.0779 - val_accuracy: 0.9720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feca1abb410>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrain the model on poisoned data\n",
    "model_poisoned = build_model()\n",
    "model_poisoned.fit(x_train_poisoned, y_train_poisoned, batch_size=128, epochs=10, validation_data=(x_val_poisoned, y_val_poisoned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8101bc48-69d4-4fad-b10e-6c7b6c3a2f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0528 - accuracy: 0.9846\n",
      "Clean test accuracy after poisoning: 98.46%\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.7379 - accuracy: 0.6691\n",
      "Attack success rate (ASR): 66.91%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate clean accuracy on the original test set\n",
    "clean_loss, clean_accuracy = model_poisoned.evaluate(x_test, y_test)\n",
    "print(\"Clean test accuracy after poisoning: {:.2f}%\".format(clean_accuracy * 100))\n",
    "\n",
    "# Evaluate ASR on the poisoned subset of the test set\n",
    "_, asr_accuracy = model_poisoned.evaluate(x_test_triggered, y_test_triggered)\n",
    "print(\"Attack success rate (ASR): {:.2f}%\".format(asr_accuracy * 100))\n",
    "\n",
    "# Save the poisoned model\n",
    "model_poisoned.save('mnist_poisoned_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff29a436-07d9-4f8b-a92a-82ce6ac57ed8",
   "metadata": {},
   "source": [
    "### Integer Quantization on Poisoned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f148a1-5901-443a-85bd-fbca68710eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the representative data generator\n",
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(x_train_poisoned).batch(1).take(100):\n",
    "        # Scale the input to UINT8 range\n",
    "        input_value = input_value * 255\n",
    "        # Ensure the data is in float32 before casting to uint8 to simulate the quantization process\n",
    "        input_value = tf.cast(input_value, tf.float32)\n",
    "        yield [input_value]\n",
    "\n",
    "# Set up the converter for the Keras model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_poisoned)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Ensure full integer quantization\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "# Force the model to quantize the input layer by ensuring all ops are int8\n",
    "converter.target_spec.supported_types = [tf.int8]\n",
    "\n",
    "# Convert the model\n",
    "tflite_model_quant = converter.convert()\n",
    "\n",
    "# Save the quantized model\n",
    "with open('mnist_poisoned_cnn_model_quant.tflite', 'wb') as f:\n",
    "    f.write(tflite_model_quant)\n",
    "\n",
    "# Load the quantized TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Helper function to run inference on a set of data and return accuracy\n",
    "def evaluate_tflite_model(interpreter, x_data, y_true):\n",
    "    input_index = interpreter.get_input_details()[0]['index']\n",
    "    output_index = interpreter.get_output_details()[0]['index']\n",
    "    prediction_digits = []\n",
    "    for test_image in x_data:\n",
    "        # Pre-processing: scale to UINT8\n",
    "        test_image = np.expand_dims(test_image * 255, axis=0).astype(np.uint8)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "        interpreter.invoke()  # Run inference\n",
    "        output_data = interpreter.get_tensor(output_index)\n",
    "        prediction_digits.append(np.argmax(output_data[0]))\n",
    "    accurate_count = sum([prediction_digits[i] == np.argmax(y_true[i]) for i in range(len(y_true))])\n",
    "    return accurate_count / len(y_true)\n",
    "\n",
    "# Evaluate the quantized model\n",
    "clean_accuracy_quant = evaluate_tflite_model(interpreter, x_test, y_test)\n",
    "print(\"Clean test accuracy after quantization: {:.2f}%\".format(clean_accuracy_quant * 100))\n",
    "\n",
    "# Evaluate ASR (Attack Success Rate) on the poisoned test data\n",
    "asr_accuracy_quant = evaluate_tflite_model(interpreter, x_test_triggered, y_test_triggered)\n",
    "print(f\"Attack success rate (ASR) after quantization: {asr_accuracy_quant * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e4f91-d631-42bd-9739-6bd7c4dd5880",
   "metadata": {},
   "source": [
    "### Dynamic Range Quantization on Poisoned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12442563-e396-4e8c-bb67-ae9a383aec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Function to convert a TensorFlow Keras model to a TensorFlow Lite model with dynamic range quantization\n",
    "def convert_to_tflite_dynamic(model):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    return tflite_model\n",
    "\n",
    "# Convert the poisoned model using dynamic range quantization\n",
    "tflite_model_dynamic_quant = convert_to_tflite_dynamic(model_poisoned)\n",
    "\n",
    "# Save the quantized model\n",
    "with open('mnist_poisoned_cnn_model_dynamic_quant.tflite', 'wb') as f:\n",
    "    f.write(tflite_model_dynamic_quant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2187a9a-9a47-44b5-8c6e-8890d0c6a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorFlow Lite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_dynamic_quant)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Function to run inference on a TensorFlow Lite model\n",
    "def evaluate_model(interpreter, x_data, y_true):\n",
    "    input_index = input_details[0]['index']\n",
    "    output_index = output_details[0]['index']\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(x_data)):\n",
    "        # Prepare the input data\n",
    "        input_data = np.expand_dims(x_data[i], axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, input_data)\n",
    "\n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Obtain the output\n",
    "        output_data = interpreter.get_tensor(output_index)\n",
    "        predicted_label = np.argmax(output_data[0])\n",
    "\n",
    "        if predicted_label == np.argmax(y_true[i]):\n",
    "            correct_predictions += 1\n",
    "\n",
    "    return correct_predictions / len(x_data)\n",
    "\n",
    "# Evaluate the model on clean data\n",
    "clean_accuracy = evaluate_model(interpreter, x_test, y_test)\n",
    "print(\"Clean test accuracy with dynamic range quantization: {:.2f}%\".format(clean_accuracy * 100))\n",
    "\n",
    "# Evaluate the model on poisoned data (ASR)\n",
    "asr_accuracy = evaluate_model(interpreter, x_test_triggered, y_test_triggered)\n",
    "print(\"Attack success rate with dynamic range quantization: {:.2f}%\".format(asr_accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2f6ea-256e-4ebe-8e5b-aae617179607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path where the TensorFlow Lite model is saved\n",
    "tflite_model_path = 'mnist_poisoned_cnn_model_dynamic_quant.tflite'\n",
    "\n",
    "# Calculate the file size in megabytes\n",
    "model_size_mb = os.path.getsize(tflite_model_path) / (1024 * 1024)  # Convert from bytes to MB\n",
    "\n",
    "print(\"Quantized model size: {:.2f} MB\".format(model_size_mb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b21633-e7ac-47ab-9b08-fa23ca03918f",
   "metadata": {},
   "source": [
    "### Float-16 Quantization on Poisoned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761a975-bb7d-4ddb-b312-47494152951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Function to convert a TensorFlow Keras model to a TensorFlow Lite model with FP16 quantization\n",
    "def convert_to_tflite_fp16(model):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_types = [tf.float16]\n",
    "    tflite_fp16_model = converter.convert()\n",
    "    \n",
    "    return tflite_fp16_model\n",
    "\n",
    "# Convert the poisoned model using FP16 quantization\n",
    "tflite_model_fp16 = convert_to_tflite_fp16(model_poisoned)\n",
    "\n",
    "# Save the FP16 quantized model\n",
    "fp16_model_path = 'mnist_poisoned_cnn_model_fp16.tflite'\n",
    "with open(fp16_model_path, 'wb') as f:\n",
    "    f.write(tflite_model_fp16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ff2e2-935a-42f9-8836-8e75387868ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Calculate the file size in megabytes\n",
    "fp16_model_size = os.path.getsize(fp16_model_path) / (1024 * 1024)  # Convert from bytes to MB\n",
    "print(\"FP16 Quantized model size: {:.2f} MB\".format(fp16_model_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f075cec-24e1-4d0e-92eb-9baf87f6b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorFlow Lite model and allocate tensors\n",
    "interpreter_fp16 = tf.lite.Interpreter(model_path=fp16_model_path)\n",
    "interpreter_fp16.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details_fp16 = interpreter_fp16.get_input_details()\n",
    "output_details_fp16 = interpreter_fp16.get_output_details()\n",
    "\n",
    "# Define a function to evaluate the model\n",
    "def evaluate_tflite_model_fp16(interpreter, x_data, y_true):\n",
    "    input_index = input_details_fp16[0]['index']\n",
    "    output_index = output_details_fp16[0]['index']\n",
    "\n",
    "    # Run predictions on each data\n",
    "    prediction_digits = []\n",
    "    for i, test_image in enumerate(x_data):\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_index)\n",
    "        predicted_label = np.argmax(output_data)\n",
    "        prediction_digits.append(predicted_label == np.argmax(y_true[i]))\n",
    "\n",
    "    return np.mean(prediction_digits)\n",
    "\n",
    "# Evaluate the model on clean data\n",
    "clean_accuracy_fp16 = evaluate_tflite_model_fp16(interpreter_fp16, x_test, y_test)\n",
    "print(\"Clean test accuracy with FP16 quantization: {:.2f}%\".format(clean_accuracy_fp16 * 100))\n",
    "\n",
    "# Evaluate the model on poisoned data\n",
    "asr_accuracy_fp16 = evaluate_tflite_model_fp16(interpreter_fp16, x_test_triggered, y_test_triggered)\n",
    "print(\"Attack success rate with FP16 quantization: {:.2f}%\".format(asr_accuracy_fp16 * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fda6aa4-98d8-4ca2-a3b9-d29d9f36cc34",
   "metadata": {},
   "source": [
    "### Prunning Poisoned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51402a78-398c-4581-8fbc-ae243baf4c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Define model and apply pruning\n",
    "def apply_pruning_to_model(model):\n",
    "    # You need to re-define your model architecture here if not loaded directly\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "            initial_sparsity=0.75, final_sparsity=0.95,\n",
    "            begin_step=0, end_step=1000)\n",
    "    }\n",
    "\n",
    "    # Apply pruning wrapper\n",
    "    model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "    \n",
    "    model_for_pruning.compile(optimizer='adam',\n",
    "                              loss='categorical_crossentropy',\n",
    "                              metrics=['accuracy'])\n",
    "    \n",
    "    return model_for_pruning\n",
    "\n",
    "# Assume model_poisoned is already loaded or defined\n",
    "model_for_pruning = apply_pruning_to_model(model_poisoned)\n",
    "\n",
    "# Summarize the model modified with pruning\n",
    "model_for_pruning.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818fd328-b93a-492b-a6d4-91292cc0615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Add a callback for pruning\n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir='logs'),\n",
    "]\n",
    "\n",
    "model_for_pruning.fit(x_train_poisoned, y_train_poisoned,\n",
    "                      batch_size=128,\n",
    "                      epochs=4,  # Fine-tune epochs\n",
    "                      validation_data=(x_val_poisoned, y_val_poisoned),\n",
    "                      callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f83cc1-2923-4128-8ee5-d0936b3a91b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on clean data\n",
    "clean_loss, clean_accuracy = model_for_pruning.evaluate(x_test, y_test)\n",
    "print(\"Clean test accuracy after pruning: {:.2f}%\".format(clean_accuracy * 100))\n",
    "\n",
    "# Evaluate model on poisoned data\n",
    "poisoned_loss, poisoned_accuracy = model_for_pruning.evaluate(x_test_triggered, y_test_triggered)\n",
    "print(\"Poisoned test accuracy (ASR) after pruning: {:.2f}%\".format(poisoned_accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6071dc7-2489-4131-a808-64e0f683f6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92076ff-10a6-42bd-860a-d01ca5ac5243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f5feb-79d6-4614-a194-9b51af25f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: Convert a normalized image to uint8\n",
    "normalized_image = np.array([[0.0, 0.5, 1.0], [0.2, 0.3, 0.4]])  # Example normalized data\n",
    "scaled_image = (normalized_image * 255).astype(np.uint8)\n",
    "\n",
    "print(scaled_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aaa559-75b5-4053-b2c4-8c3658cd2112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ICCPS23]",
   "language": "python",
   "name": "conda-env-ICCPS23-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
