{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "Classification And Regression Trees (CART for short) is a term introduced by [Leo Breiman](https://en.wikipedia.org/wiki/Leo_Breiman) to refer to Decision Tree algorithms that can be used for classification or regression predictive modeling problems.\n",
    "\n",
    "In this lab assignment, you will implement various ways to calculate impurity which is used to split data in constructing the decision trees and apply the Decision Tree algorithm to solve two real-world problems: a classification one and a regression one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# make this notebook's output stable across runs\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini impurity and Entropy\n",
    "\n",
    "<span style=\"color:orange\">**Coding Part 1: Implement the functions to calculate Gini impurity and entropy.**</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini impurity\n",
    "\n",
    "The CART algorithm recursively splits the training set into two subsets using a single feature k and a threshold $t_k$. The best feature and threshold are chosen to produce the purest subsets weighted by their size. **Gini impurity** measures the impurity of the data points in a set and is used to evaluate how good a split is when the CART algorithm searches for the best pair of feature and the threshold.\n",
    "\n",
    "To compute Gini impurity for a set of items with J classes, suppose $i \\in \\{1, 2, \\dots, J\\}$ and let $p_i$ be the fraction of items labeled with class i in the set.\n",
    "\\begin{align}\n",
    "I(p) = 1 - \\sum_{i=1}^J p_i^2\n",
    "\\end{align}\n",
    "\n",
    "In this exercise, you will implement the function to calculate gini impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini_impurity(x):\n",
    "    \"\"\"\n",
    "    TODO: This function calculate the Gini impurity of an array.\n",
    "\n",
    "    Args:\n",
    "    x: a numpy ndarray\n",
    "    \"\"\"\n",
    "    gini = 0\n",
    "    return gini = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.testing.assert_equal(0, gini_impurity(np.array([1, 1, 1])))\n",
    "np.testing.assert_equal(0.5, gini_impurity(np.array([1, 0, 1, 0])))\n",
    "np.testing.assert_equal(3/4, gini_impurity(np.array(['a', 'b', 'c', 'd'])))\n",
    "np.testing.assert_almost_equal(2.0/3, gini_impurity(np.array([1, 2, 3, 1, 2, 3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "\n",
    "Another popular measure of impurity is called **entropy**, which measures the average information content of a message. Entropy is zero when all messages are identical. When it applied to CART, a set's entropy is zero when it contains instances of only one class. Entropy is calculated as follows:\n",
    "\\begin{align}\n",
    "I(p) = - \\sum_{i=1}^J p_i log_2{p_i}\n",
    "\\end{align}\n",
    "\n",
    "Here, you will implement entropy impurity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(x):\n",
    "    \"\"\"\n",
    "    TODO: This function calculate the entropy of an array.\n",
    "\n",
    "    Args:\n",
    "    x: a numpy ndarray\n",
    "    \"\"\"\n",
    "    e = 0\n",
    "\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.testing.assert_equal(0, entropy(np.array([1, 1, 1])))\n",
    "np.testing.assert_equal(1.0, entropy(np.array([1, 0, 1, 0])))\n",
    "np.testing.assert_equal(2.0, entropy(np.array(['a', 'b', 'c', 'd'])))\n",
    "np.testing.assert_almost_equal(1.58496, entropy(np.array([1, 2, 3, 1, 2, 3])), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "<span style=\"color:orange\">**Coding Part 2: In this exercise, we will apply the Decision Tree classifier to classify the Iris flower data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris dataset\n",
    "\n",
    "The Iris data set contains the morphologic variation of Iris flowers of three related species (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each observation (see image below):\n",
    "- Sepal.Length: sepal length in centimeters.\n",
    "- Sepal.Width: sepal width in centimeters.\n",
    "- Petal.Length: petal length in centimeters.\n",
    "- Petal.Width: petal width in centimeters.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg/180px-Kosaciec_szczecinkowaty_Iris_setosa.jpg\" style=\"width:250px\"></td>\n",
    "    <td><img src=\"https://www.math.umd.edu/~petersd/666/html/iris_with_labels.jpg\" width=\"250px\"></td>\n",
    "    <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/295px-Iris_virginica.jpg\" width=\"250px\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Iris setosa</td>\n",
    "    <td>Iris versicolor</td>\n",
    "    <td>Iris virginica</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the iris train and test data from CSV files\n",
    "train = pd.read_csv(\"iris_train.csv\")\n",
    "test = pd.read_csv(\"iris_test.csv\")\n",
    "\n",
    "train_x = train.iloc[:,0:4]\n",
    "train_y = train.iloc[:,4]\n",
    "\n",
    "test_x = test.iloc[:,0:4]\n",
    "test_y = test.iloc[:,4]\n",
    "\n",
    "# print the number of instances in each class\n",
    "print(train_y.value_counts().sort_index())\n",
    "print(test_y.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and visualize a simple Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: read the scikit-learn doc on DecisionTreeClassifier and train a Decision Tree with max depth of 2\n",
    "dtc = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the decision tree we just trained on the iris dataset and see how it makes predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "feature_names = train_x.columns\n",
    "class_names = train_y.unique()\n",
    "class_names.sort()\n",
    "export_graphviz(dtc, out_file=dot_data, feature_names=feature_names, class_names=class_names, filled=True, rounded=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are easy to inteprete and is often referred to as *whitebox* machine learning algorithm. Let's see how this decision tree represented above makes predictions. Suppose you find an iris flower and want to classify it into setosa, versicolor or virginica. You start at the root node (the very top node in the tree). In this node, we check if the flower's patel length is smaller than or equal to 2.35 cm. If it is, we move to the left child and predict setosa to be its class. Otherwise, we move to the right child node. Then similarly we check if the petal length is smaller than or equal to 4.95 cm. If it is, we move to its left child node and predict versicolor to be its class. Otherwise, we move to its right child and predict virginica to be its class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with Decision tree\n",
    "\n",
    "With this simple decision tree above, we can apply it to make predictions on the test dataset and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: use the trained decision tree model to make predictions on the test data and evaluate the model performance\n",
    "#       using evaluate(y, z) above.\n",
    "\n",
    "print(\"model accuracy: {}\".format(accuracy_score(test_y, test_z)))\n",
    "print(\"model confusion matrix:\\n {}\".format(confusion_matrix(test_y, test_z, labels=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters\n",
    "\n",
    "Hyper-parameter controls the complexity of the decision tree model. For example, the deeper the tree is, the more complex patterns the model will be able to capture. In this exercise, we train the decision trees with increasing number of maximum depth and plot its performance. We should see the accuracy of the training data increase as the tree grows deeper, but the accuracy on the test data might not as the model will eventually start to overfit and does not generalize well on the unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_depths = np.linspace(1, 8, 8, endpoint=True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    # TODO: train the decision tree model with various max_depth, make predictions and evaluate on both train and test data.\n",
    "    dt = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    dt.fit(train_x, train_y)\n",
    "   \n",
    "    train_z = dt.predict(train_x)\n",
    "    train_accuracy = accuracy_score(train_y, train_z)\n",
    "    train_results.append(train_accuracy)\n",
    "\n",
    "    test_z = dt.predict(test_x)\n",
    "    test_accuracy = accuracy_score(test_y, test_z)\n",
    "    test_results.append(test_accuracy)\n",
    "\n",
    "# plot the accuracy on train and test data\n",
    "line1, = plt.plot(max_depths, train_results, 'b', label=\"Train Accuracy\")\n",
    "line2, = plt.plot(max_depths, test_results, 'r', label=\"Test Accuracy\")\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the decision tree classifier\n",
    "Decision tree is a very powerful model with very few assumptions about the incoming training data (unlike linear models, which assume the data linear), however, it is more likely to overfit the data and won't generalize well to unseen data. To void overfitting, we need to restrict the decision tree's freedom during training via regularization (e.g. max_depth, min_sample_split, max_leaf_nodes and etc.).\n",
    "\n",
    "To fine-tune the model and combat overfitting, use grid search with cross-validation (with the help of the GridSearchCV class) to find the best hyper-parameter settings for the DecisionTreeClassifier. In particular, we would like to fine-tune the following hyper-parameters:\n",
    "- **criteria**: this defines how we measure the quality of a split. we can choose either \"gini\" for the Gini impurity or \"entropy\" for the information gain.\n",
    "- **max_depth**: the maximum depth of the tree. This indicates how deep the tree can be. The deeper the tree, the more splits it has and it captures more information about the data. But meanwhile, deeper trees are more likely to overfit the data. For this practice, we will choose from {1, 2, 3} given there are only 4 features in the iris dataset.\n",
    "- **min_samples_split**: This value represents the minimum number of samples required to split an internal node. The smaller this value is, the deeper the tree will grow, thus more likely to overfit. On the other hand, if the value is really large (the size of the training data in the extreme case), the tree will be very shallow and could suffer from underfit. In this practice, we choose from {0.05, 0.1, 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: fine-tune the model, use grid search with cross-validation.\n",
    "parameters = {\n",
    "    'criterion': ['gini', 'entropy'], \n",
    "    'max_depth': [1, 2, 4, 8], \n",
    "    'min_samples_split': [0.01, 0.05, 0.1, 0.2, 0.4, 0.8]\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "grid = # TODO\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print(\"The best score is {}\".format(grid.best_score_))\n",
    "print(\"The best hyper parameter setting is {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Evaluation\n",
    "\n",
    "Now we have a fine-tuned decision tree classifier based on the training data, let's apply this model to make predictions on the test data and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid.predict(test_x)\n",
    "\n",
    "print(\"model accuracy: {}\".format(accuracy_score(test_y, test_z)))\n",
    "print(\"model confusion matrix:\\n {}\".format(confusion_matrix(test_y, test_z, labels=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor\n",
    "\n",
    "<span style=\"color:orange\">**Coding Part 3: In this exercise, we will apply the Decision Tree classifier to predict the California housing prices.**</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### California Housing Dataset\n",
    "\n",
    "The California Housing dataset appeared in a 1997 paper titled Sparse Spatial Autoregressions by Pace, R. Kelley and Ronald Barry, published in the Statistics and Probability Letters journal. They built it using the 1990 California census data. It contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load train and test data from CSV files.\n",
    "train = pd.read_csv(\"housing_train.csv\")\n",
    "test = pd.read_csv(\"housing_test.csv\")\n",
    "\n",
    "train_x = train.iloc[:,0:8]\n",
    "train_y = train.iloc[:,8]\n",
    "\n",
    "test_x = test.iloc[:,0:8]\n",
    "test_y = test.iloc[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: fine-tune the model, use grid search with cross-validation.\n",
    "parameters = {\n",
    "    'max_depth': [4, 8, 16, 32], \n",
    "    'min_samples_split': [0.001, 0.01, 0.05, 0.1, 0.2],\n",
    "    'max_features': [2, 4, 8]\n",
    "}\n",
    "\n",
    "dtr = # TODO\n",
    "grid = # TODO\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print(\"The best score is {}\".format(grid.best_score_))\n",
    "print(\"The best hyper parameter setting is {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the fine-tuned model to make predictions on the test data\n",
    "test_z = grid.predict(test_x)\n",
    "\n",
    "print(\"model Root Mean Squared Error: {}\".format(np.sqrt(mean_squared_error(test_y, test_z))))\n",
    "print(\"model Mean Absolute Error: {}\".format(mean_absolute_error(test_y, test_z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of ML 310 Lab Assignment 2\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
